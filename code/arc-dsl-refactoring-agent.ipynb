{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ba2755ca",
   "metadata": {},
   "source": [
    "## ðŸš€ Quick Start Guide\n",
    "\n",
    "**Before running this notebook:**\n",
    "\n",
    "1. **Get a Gemini API Key**\n",
    "   - Visit [Google AI Studio](https://aistudio.google.com/app/api-keys)\n",
    "   - Click \"Create API Key\"\n",
    "   - Copy your API key\n",
    "\n",
    "2. **Create `.env` file**\n",
    "   - In this directory (`/code/`), create a file named `.env`\n",
    "   - Add the following line:\n",
    "   ```\n",
    "   GOOGLE_API_KEY=your_actual_api_key_here\n",
    "   ```\n",
    "   - Save the file\n",
    "\n",
    "3. **Run the notebook**\n",
    "   - Execute cells in order from top to bottom\n",
    "   - The system will load your API key automatically\n",
    "   - Interactive HITL checkpoints will prompt for approval/rejection\n",
    "\n",
    "**Note:** A `.env.example` file is provided as a template."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2e5860d",
   "metadata": {},
   "source": [
    "# HITL Multi-Agent Code Refactoring System\n",
    "\n",
    "**Project:** ARC-DSL Refactoring Agent System  \n",
    "**Track:** Kaggle Agents Intensive - Freestyle  \n",
    "**Date:** November 18, 2025\n",
    "\n",
    "## Overview\n",
    "\n",
    "A human-in-the-loop (HITL) multi-agent system that incrementally refactors the [arc-dsl codebase](https://github.com/michaelhodel/arc-dsl) through intelligent analysis, proposal generation, validation, and documentation.\n",
    "\n",
    "**Core Philosophy:** Humans approve strategy, agents execute tactics.\n",
    "\n",
    "### Key Features\n",
    "\n",
    "- **5 Specialized Agents:** Coordinator, Analysis, Refactor, Validation, Documentation\n",
    "- **Custom Tools:** File I/O, code analysis, refactoring utilities, testing\n",
    "- **HITL Approval:** Interactive checkpoints for human oversight\n",
    "- **Session Management:** Track progress across files and iterations\n",
    "- **Memory Bank:** Learn from human approval patterns\n",
    "- **Observability:** LoggingPlugin for traces and metrics\n",
    "- **Gemini-Powered:** All agents use Gemini 2.5 Flash Lite\n",
    "\n",
    "### Refactoring Goals\n",
    "\n",
    "1. **Reduce Type Ambiguity:** Eliminate Union types, remove isinstance checks\n",
    "2. **Group Functions by Signature:** Create triage functions for better organization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cb0fd2fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ arc-dsl repository already exists\n"
     ]
    }
   ],
   "source": [
    "# Install required packages\n",
    "!pip install -q google-genai google-adk ipywidgets\n",
    "\n",
    "# Clone arc-dsl repository if not already present\n",
    "import os\n",
    "if not os.path.exists('arc-dsl'):\n",
    "    !git clone https://github.com/michaelhodel/arc-dsl.git\n",
    "    print(\"âœ“ arc-dsl repository cloned\")\n",
    "else:\n",
    "    print(\"âœ“ arc-dsl repository already exists\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73613729",
   "metadata": {},
   "source": [
    "## Section 1: Import Libraries\n",
    "\n",
    "Import all necessary libraries for the multi-agent system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7afc40a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install -q python-dotenv google-genai google-adk ipywidgets\n",
    "\n",
    "# Clone arc-dsl repository if not already present\n",
    "import os\n",
    "if not os.path.exists('arc-dsl'):\n",
    "    !git clone https://github.com/michaelhodel/arc-dsl.git\n",
    "    print(\"âœ“ arc-dsl repository cloned\")\n",
    "else:\n",
    "    print(\"âœ“ arc-dsl repository already exists\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df1b4a72",
   "metadata": {},
   "source": [
    "## Section 2: Configure Gemini API Key\n",
    "\n",
    "Load the Gemini API key from the .env file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b8abb43",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "try:\n",
    "    # Load environment variables from .env file\n",
    "    load_dotenv()\n",
    "    GOOGLE_API_KEY = os.getenv(\"GOOGLE_API_KEY\")\n",
    "    os.environ[\"GOOGLE_API_KEY\"] = GOOGLE_API_KEY\n",
    "    print(\"âœ… Gemini API key setup complete.\")\n",
    "except Exception as e:\n",
    "    print(\n",
    "        f\"ðŸ”‘ Authentication Error: Please make sure you have added 'GOOGLE_API_KEY' to your .env file. Details: {e}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04b83c26",
   "metadata": {},
   "source": [
    "## Section 3: Configure Gemini Client\n",
    "\n",
    "Initialize the Gemini client with the API key."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd1b9abd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure Gemini API\n",
    "MODEL_NAME = 'gemini-2.0-flash-exp'  # Using Gemini 2.0 Flash\n",
    "\n",
    "# Initialize Gemini client\n",
    "client = genai.Client(api_key=os.environ.get(\"GOOGLE_API_KEY\"))\n",
    "\n",
    "# Test connection\n",
    "try:\n",
    "    response = client.models.generate_content(\n",
    "        model=MODEL_NAME,\n",
    "        contents=\"Hello! Please confirm you're working.\"\n",
    "    )\n",
    "    print(f\"âœ… Gemini API configured successfully\")\n",
    "    print(f\"   Model: {MODEL_NAME}\")\n",
    "    print(f\"   Response: {response.text[:100]}...\")\n",
    "except Exception as e:\n",
    "    print(f\"âš ï¸  Gemini API configuration error: {e}\")\n",
    "    print(\"   Please check your GOOGLE_API_KEY in .env file\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bdd834d",
   "metadata": {},
   "source": [
    "## Section 4: Define Custom Tools\n",
    "\n",
    "Create custom tools for file operations, code analysis, and testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72972900",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard library imports\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "import shutil\n",
    "import subprocess\n",
    "import ast\n",
    "import copy\n",
    "import random\n",
    "from datetime import datetime\n",
    "from collections import defaultdict\n",
    "from typing import Any, Dict, List, Tuple, Optional\n",
    "\n",
    "# ADK imports (following course patterns)\n",
    "from google import genai\n",
    "from google.genai import types\n",
    "\n",
    "# For demonstration - actual ADK imports would be:\n",
    "# from google.adk import InMemoryRunner, InMemorySessionService, MemoryBank, LoggingPlugin\n",
    "# Since we're demonstrating the pattern, we'll create mock implementations\n",
    "\n",
    "# ipywidgets for HITL interface\n",
    "try:\n",
    "    from ipywidgets import Button, VBox, HBox, HTML, Textarea\n",
    "    from IPython.display import display, clear_output\n",
    "    IPYWIDGETS_AVAILABLE = True\n",
    "except ImportError:\n",
    "    IPYWIDGETS_AVAILABLE = False\n",
    "    print(\"âš  ipywidgets not available, will use simple input() interface\")\n",
    "\n",
    "print(\"âœ“ All libraries imported successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7516a217",
   "metadata": {},
   "source": [
    "## Section 5: Initialize Memory Bank and Session Service\n",
    "\n",
    "Set up memory bank for learning from human decisions and session management."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb451110",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom Tools Implementation\n",
    "\n",
    "class RefactoringTools:\n",
    "    \"\"\"Collection of custom tools for code refactoring\"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def read_file(file_path: str) -> str:\n",
    "        \"\"\"Read contents of a source file.\"\"\"\n",
    "        try:\n",
    "            with open(file_path, 'r') as f:\n",
    "                return f.read()\n",
    "        except Exception as e:\n",
    "            return f\"Error reading file: {e}\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def write_file(file_path: str, content: str) -> str:\n",
    "        \"\"\"Write content to a file (with backup).\"\"\"\n",
    "        try:\n",
    "            # Create backup\n",
    "            if os.path.exists(file_path):\n",
    "                backup_path = f\"{file_path}.backup.{datetime.now().strftime('%Y%m%d_%H%M%S')}\"\n",
    "                shutil.copy(file_path, backup_path)\n",
    "                backup_msg = f\", backup at {backup_path}\"\n",
    "            else:\n",
    "                backup_msg = \"\"\n",
    "            \n",
    "            # Write new content\n",
    "            with open(file_path, 'w') as f:\n",
    "                f.write(content)\n",
    "            \n",
    "            return f\"âœ“ Written to {file_path}{backup_msg}\"\n",
    "        except Exception as e:\n",
    "            return f\"Error writing file: {e}\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def analyze_type_usage(file_path: str) -> Dict:\n",
    "        \"\"\"Find isinstance checks and Union types in Python file.\"\"\"\n",
    "        try:\n",
    "            with open(file_path, 'r') as f:\n",
    "                tree = ast.parse(f.read())\n",
    "            \n",
    "            isinstance_calls = []\n",
    "            union_types = []\n",
    "            \n",
    "            for node in ast.walk(tree):\n",
    "                if isinstance(node, ast.Call):\n",
    "                    if getattr(node.func, 'id', None) == 'isinstance':\n",
    "                        isinstance_calls.append({\n",
    "                            'line': node.lineno,\n",
    "                            'args': [ast.unparse(arg) for arg in node.args]\n",
    "                        })\n",
    "                \n",
    "                if isinstance(node, ast.Subscript):\n",
    "                    if ast.unparse(node.value) == 'Union':\n",
    "                        union_types.append({\n",
    "                            'line': node.lineno,\n",
    "                            'definition': ast.unparse(node)\n",
    "                        })\n",
    "            \n",
    "            return {\n",
    "                'isinstance_checks': isinstance_calls,\n",
    "                'union_types': union_types,\n",
    "                'total_isinstance': len(isinstance_calls),\n",
    "                'total_unions': len(union_types)\n",
    "            }\n",
    "        except Exception as e:\n",
    "            return {'error': str(e)}\n",
    "    \n",
    "    @staticmethod\n",
    "    def find_function_signatures(file_path: str) -> Dict:\n",
    "        \"\"\"Identify functions with identical signatures for grouping.\"\"\"\n",
    "        try:\n",
    "            with open(file_path, 'r') as f:\n",
    "                tree = ast.parse(f.read())\n",
    "            \n",
    "            signature_groups = defaultdict(list)\n",
    "            \n",
    "            for node in ast.walk(tree):\n",
    "                if isinstance(node, ast.FunctionDef):\n",
    "                    # Extract signature\n",
    "                    params = [arg.annotation for arg in node.args.args if arg.annotation]\n",
    "                    returns = node.returns\n",
    "                    \n",
    "                    if params and returns:\n",
    "                        sig = f\"({', '.join(ast.unparse(p) for p in params)}) -> {ast.unparse(returns)}\"\n",
    "                        signature_groups[sig].append(node.name)\n",
    "            \n",
    "            # Filter to groups with 2+ functions\n",
    "            groupable = {sig: funcs for sig, funcs in signature_groups.items() if len(funcs) >= 2}\n",
    "            \n",
    "            return {\n",
    "                'total_signatures': len(signature_groups),\n",
    "                'groupable_signatures': len(groupable),\n",
    "                'groups': groupable\n",
    "            }\n",
    "        except Exception as e:\n",
    "            return {'error': str(e)}\n",
    "    \n",
    "    @staticmethod\n",
    "    def run_tests(test_file: Optional[str] = None) -> Dict:\n",
    "        \"\"\"Run pytest on specified test file or entire suite.\"\"\"\n",
    "        try:\n",
    "            cmd = ['pytest', '-v', '--tb=short']\n",
    "            if test_file:\n",
    "                cmd.append(test_file)\n",
    "            \n",
    "            result = subprocess.run(cmd, capture_output=True, text=True, cwd='arc-dsl')\n",
    "            \n",
    "            # Parse pytest output\n",
    "            lines = result.stdout.split('\\n')\n",
    "            passed = failed = 0\n",
    "            for line in lines:\n",
    "                if ' passed' in line:\n",
    "                    try:\n",
    "                        passed = int(line.split()[0])\n",
    "                    except:\n",
    "                        pass\n",
    "                if ' failed' in line:\n",
    "                    try:\n",
    "                        failed = int(line.split()[0])\n",
    "                    except:\n",
    "                        pass\n",
    "            \n",
    "            return {\n",
    "                'exit_code': result.returncode,\n",
    "                'passed': passed,\n",
    "                'failed': failed,\n",
    "                'output': result.stdout[:1000],  # Truncate for display\n",
    "                'success': result.returncode == 0\n",
    "            }\n",
    "        except Exception as e:\n",
    "            return {'error': str(e), 'success': False}\n",
    "\n",
    "# Initialize tools\n",
    "tools = RefactoringTools()\n",
    "\n",
    "print(\"âœ“ Custom tools defined:\")\n",
    "print(\"  - read_file, write_file\")\n",
    "print(\"  - analyze_type_usage, find_function_signatures\")\n",
    "print(\"  - run_tests\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86f58ef8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Memory Bank: Learn from human approval patterns\n",
    "memory_bank = {\n",
    "    'approval_patterns': [],\n",
    "    'rejection_reasons': [],\n",
    "    'preferences': {\n",
    "        'incremental_changes': True,\n",
    "        'backward_compatibility': True,\n",
    "        'test_all_solvers': True\n",
    "    }\n",
    "}\n",
    "\n",
    "# Session State: Track refactoring progress\n",
    "session_state = {\n",
    "    'session_id': f\"refactor_arc_dsl_{datetime.now().strftime('%Y%m%d_%H%M%S')}\",\n",
    "    'start_time': datetime.now(),  # Store as datetime object for duration calculations\n",
    "    'current_file': None,\n",
    "    'files_to_process': ['arc-dsl/constants.py', 'arc-dsl/arc_types.py', 'arc-dsl/dsl.py'],\n",
    "    'files_completed': [],\n",
    "    'total_proposals': 0,\n",
    "    'approved_proposals': 0,\n",
    "    'rejected_proposals': 0,\n",
    "    'modified_proposals': 0,\n",
    "    'metrics': {\n",
    "        'isinstance_checks_removed': 0,\n",
    "        'union_types_eliminated': 0,\n",
    "        'functions_grouped': 0,\n",
    "        'lines_added': 0,\n",
    "        'lines_removed': 0,\n",
    "        'tests_passed': 0,\n",
    "        'test_coverage': 0.0  # Initialize test coverage metric\n",
    "    },\n",
    "    'checkpoints': []\n",
    "}\n",
    "\n",
    "def update_session(key: str, value: Any):\n",
    "    \"\"\"Update session state and display progress\"\"\"\n",
    "    session_state[key] = value\n",
    "    print(f\"ðŸ“Š Session updated: {key} = {value}\")\n",
    "\n",
    "def query_memory(context: str) -> List[Dict]:\n",
    "    \"\"\"Query memory bank for relevant patterns\"\"\"\n",
    "    return [p for p in memory_bank['approval_patterns'] if context.lower() in p.get('context', '').lower()]\n",
    "\n",
    "def store_memory(memory_type: str, data: Dict):\n",
    "    \"\"\"Store decision in memory bank for learning\"\"\"\n",
    "    if memory_type == 'approval':\n",
    "        memory_bank['approval_patterns'].append(data)\n",
    "    elif memory_type == 'rejection':\n",
    "        memory_bank['rejection_reasons'].append(data)\n",
    "    print(f\"ðŸ’¾ Memory stored: {memory_type}\")\n",
    "\n",
    "print(\"âœ“ Memory Bank and Session Service initialized\")\n",
    "print(f\"  Session ID: {session_state['session_id']}\")\n",
    "print(f\"  Files to process: {len(session_state['files_to_process'])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deaeaf5e",
   "metadata": {},
   "source": [
    "## Section 6: Create Specialized Agents\n",
    "\n",
    "Create agents for analysis, refactoring, validation, and documentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9925f649",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Agent System Implementation\n",
    "\n",
    "class RefactoringAgent:\n",
    "    \"\"\"Base class for refactoring agents\"\"\"\n",
    "    \n",
    "    def __init__(self, name: str, system_prompt: str):\n",
    "        self.name = name\n",
    "        self.system_prompt = system_prompt\n",
    "        self.client = client\n",
    "        self.model = MODEL_NAME\n",
    "    \n",
    "    def call(self, prompt: str, context: Dict = None) -> str:\n",
    "        \"\"\"Call agent with prompt and context\"\"\"\n",
    "        full_prompt = f\"{self.system_prompt}\\n\\n{prompt}\"\n",
    "        \n",
    "        if context:\n",
    "            full_prompt += f\"\\n\\nContext:\\n{json.dumps(context, indent=2)}\"\n",
    "        \n",
    "        try:\n",
    "            response = self.client.models.generate_content(\n",
    "                model=self.model,\n",
    "                contents=full_prompt\n",
    "            )\n",
    "            return response.text\n",
    "        except Exception as e:\n",
    "            return f\"Error calling {self.name}: {e}\"\n",
    "\n",
    "# Analysis Agent\n",
    "analysis_agent = RefactoringAgent(\n",
    "    name=\"Analysis Agent\",\n",
    "    system_prompt=\"\"\"You are the Analysis Agent specializing in Python code analysis.\n",
    "\n",
    "Your responsibilities:\n",
    "1. Analyze Python files for refactoring opportunities\n",
    "2. Identify type ambiguity (Union types, isinstance checks)\n",
    "3. Find functions with identical signatures that could be grouped\n",
    "4. Detect code smells and complexity issues\n",
    "5. Assess dependencies and impact radius\n",
    "\n",
    "Output format (JSON):\n",
    "{\n",
    "  \"issues\": [{\"type\": \"type_ambiguity\", \"location\": \"line X\", \"severity\": \"high\", \"description\": \"...\"}],\n",
    "  \"grouping_opportunities\": [{\"signature\": \"...\", \"functions\": [...], \"triage_name\": \"...\"}],\n",
    "  \"recommendations\": [{\"priority\": 1, \"issue\": \"...\", \"proposed_fix\": \"...\", \"risk_level\": \"...\"}]\n",
    "}\"\"\"\n",
    ")\n",
    "\n",
    "# Refactor Agent\n",
    "refactor_agent = RefactoringAgent(\n",
    "    name=\"Refactor Agent\",\n",
    "    system_prompt=\"\"\"You are the Refactor Agent specializing in Python code transformations.\n",
    "\n",
    "Your responsibilities:\n",
    "1. Generate concrete refactoring proposals based on analysis\n",
    "2. Create before/after code snippets\n",
    "3. Ensure backward compatibility\n",
    "4. Follow Python best practices (PEP 8, type hints)\n",
    "5. Generate small, incremental, testable changes\n",
    "\n",
    "Requirements:\n",
    "- INCREMENTAL: Small changes, not big rewrites\n",
    "- BACKWARD COMPATIBLE: Maintain existing signatures via wrappers\n",
    "- TYPE SAFE: Eliminate isinstance checks where possible\n",
    "- DOCUMENTED: Include docstrings\n",
    "\n",
    "Output format (JSON):\n",
    "{\n",
    "  \"proposal_id\": \"refactor_001\",\n",
    "  \"target\": \"Issue to address\",\n",
    "  \"strategy\": \"Approach description\",\n",
    "  \"changes\": [{\"file\": \"...\", \"before\": \"...\", \"after\": \"...\", \"lines_changed\": N}],\n",
    "  \"tests_required\": [...],\n",
    "  \"estimated_time\": \"...\"\n",
    "}\"\"\"\n",
    ")\n",
    "\n",
    "# Validation Agent\n",
    "validation_agent = RefactoringAgent(\n",
    "    name=\"Validation Agent\",\n",
    "    system_prompt=\"\"\"You are the Validation Agent responsible for testing refactored code.\n",
    "\n",
    "Your responsibilities:\n",
    "1. Verify proposed changes don't break existing functionality\n",
    "2. Check backward compatibility\n",
    "3. Recommend test cases for new code\n",
    "4. Assess risks\n",
    "\n",
    "Output format (JSON):\n",
    "{\n",
    "  \"validation_results\": {\n",
    "    \"backward_compatible\": true/false,\n",
    "    \"risks\": [...],\n",
    "    \"test_recommendations\": [...]\n",
    "  },\n",
    "  \"overall_status\": \"PASS/FAIL\",\n",
    "  \"recommendation\": \"Safe to apply / Needs revision\"\n",
    "}\"\"\"\n",
    ")\n",
    "\n",
    "# Documentation Agent\n",
    "documentation_agent = RefactoringAgent(\n",
    "    name=\"Documentation Agent\",\n",
    "    system_prompt=\"\"\"You are the Documentation Agent responsible for maintaining clear documentation.\n",
    "\n",
    "Your responsibilities:\n",
    "1. Generate docstrings for refactored functions\n",
    "2. Create migration guides if needed\n",
    "3. Document changes in changelog format\n",
    "\n",
    "Output format (JSON):\n",
    "{\n",
    "  \"docstrings\": {\"function_name\": \"docstring text\"},\n",
    "  \"changelog_entry\": \"## [Date] Description\\\\n- Changes...\",\n",
    "  \"migration_guide\": \"Text explaining how to migrate (if needed)\"\n",
    "}\"\"\"\n",
    ")\n",
    "\n",
    "print(\"âœ“ Specialized agents created:\")\n",
    "print(f\"  - {analysis_agent.name}\")\n",
    "print(f\"  - {refactor_agent.name}\")\n",
    "print(f\"  - {validation_agent.name}\")\n",
    "print(f\"  - {documentation_agent.name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e8f5de4",
   "metadata": {},
   "source": [
    "## Section 7: Create Coordinator Agent\n",
    "\n",
    "Create the coordinator agent that orchestrates the refactoring workflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37640ebb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Coordinator Agent - Orchestrates multi-agent workflow\n",
    "\n",
    "class CoordinatorAgent:\n",
    "    \"\"\"Orchestrates the refactoring workflow with HITL approval\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.client = client\n",
    "        self.model = MODEL_NAME\n",
    "    \n",
    "    def process_file(self, file_path: str) -> Dict:\n",
    "        \"\"\"Process a single file through the refactoring pipeline\"\"\"\n",
    "        print(f\"\\n{'='*80}\")\n",
    "        print(f\"ðŸ”§ PROCESSING FILE: {file_path}\")\n",
    "        print(f\"{'='*80}\\n\")\n",
    "        \n",
    "        update_session('current_file', file_path)\n",
    "        \n",
    "        # Step 1: Analysis\n",
    "        print(\"ðŸ“Š Step 1: Running Analysis Agent...\")\n",
    "        file_content = tools.read_file(file_path)\n",
    "        type_analysis = tools.analyze_type_usage(file_path)\n",
    "        sig_analysis = tools.find_function_signatures(file_path)\n",
    "        \n",
    "        analysis_prompt = f\"\"\"Analyze this file for refactoring opportunities:\n",
    "\n",
    "File: {file_path}\n",
    "Content length: {len(file_content)} characters\n",
    "\n",
    "Type Analysis:\n",
    "- isinstance checks: {type_analysis.get('total_isinstance', 0)}\n",
    "- Union types: {type_analysis.get('total_unions', 0)}\n",
    "\n",
    "Signature Analysis:\n",
    "- Total signatures: {sig_analysis.get('total_signatures', 0)}\n",
    "- Groupable signatures: {sig_analysis.get('groupable_signatures', 0)}\n",
    "\n",
    "Provide analysis focusing on:\n",
    "1. Type ambiguity issues to fix\n",
    "2. Functions that can be grouped by signature\n",
    "3. Priority recommendations\"\"\"\n",
    "        \n",
    "        analysis_result = analysis_agent.call(analysis_prompt, {\n",
    "            'file_path': file_path,\n",
    "            'type_usage': type_analysis,\n",
    "            'signatures': sig_analysis\n",
    "        })\n",
    "        \n",
    "        print(f\"âœ“ Analysis complete\\n\")\n",
    "        \n",
    "        # Step 2: Generate Refactoring Proposal\n",
    "        print(\"ðŸ”¨ Step 2: Running Refactor Agent...\")\n",
    "        refactor_prompt = f\"\"\"Based on the analysis, generate a refactoring proposal:\n",
    "\n",
    "Analysis Results:\n",
    "{analysis_result}\n",
    "\n",
    "Memory (human preferences):\n",
    "{json.dumps(memory_bank['preferences'], indent=2)}\n",
    "\n",
    "Generate ONE focused, incremental refactoring proposal.\"\"\"\n",
    "        \n",
    "        proposal = refactor_agent.call(refactor_prompt, {\n",
    "            'analysis': analysis_result,\n",
    "            'preferences': memory_bank['preferences']\n",
    "        })\n",
    "        \n",
    "        print(f\"âœ“ Proposal generated\\n\")\n",
    "        \n",
    "        # Step 3: Validation\n",
    "        print(\"âœ… Step 3: Running Validation Agent...\")\n",
    "        validation_prompt = f\"\"\"Validate this refactoring proposal:\n",
    "\n",
    "Proposal:\n",
    "{proposal}\n",
    "\n",
    "Check for:\n",
    "1. Backward compatibility\n",
    "2. Potential risks\n",
    "3. Test requirements\"\"\"\n",
    "        \n",
    "        validation_result = validation_agent.call(validation_prompt, {\n",
    "            'proposal': proposal\n",
    "        })\n",
    "        \n",
    "        print(f\"âœ“ Validation complete\\n\")\n",
    "        \n",
    "        return {\n",
    "            'file': file_path,\n",
    "            'analysis': analysis_result,\n",
    "            'proposal': proposal,\n",
    "            'validation': validation_result,\n",
    "            'type_analysis': type_analysis,\n",
    "            'sig_analysis': sig_analysis\n",
    "        }\n",
    "\n",
    "coordinator = CoordinatorAgent()\n",
    "print(\"âœ“ Coordinator Agent created\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97f1652b",
   "metadata": {},
   "source": [
    "## Section 8: Implement HITL Approval Checkpoint\n",
    "\n",
    "Implement human-in-the-loop approval mechanism for refactoring proposals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96cbe831",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main Refactoring Workflow Execution\n",
    "\n",
    "def run_refactoring_session():\n",
    "    \"\"\"Execute the full refactoring workflow with HITL approval\"\"\"\n",
    "    \n",
    "    print(f\"\\n{'#'*80}\")\n",
    "    print(f\"# STARTING REFACTORING SESSION\")\n",
    "    print(f\"# Session ID: {session_state['session_id']}\")\n",
    "    print(f\"# Files to process: {len(session_state['files_to_process'])}\")\n",
    "    print(f\"{'#'*80}\\n\")\n",
    "    \n",
    "    for file_path in session_state['files_to_process']:\n",
    "        try:\n",
    "            # Process file through analysis â†’ refactor â†’ validate pipeline\n",
    "            result = coordinator.process_file(file_path)\n",
    "            \n",
    "            # HITL Approval Checkpoint\n",
    "            decision = hitl_checkpoint(result)\n",
    "            \n",
    "            # Handle decision\n",
    "            if decision['status'] == 'approve':\n",
    "                # In a real implementation, would apply changes here\n",
    "                # For demonstration, we'll mark as completed\n",
    "                session_state['files_completed'].append(file_path)\n",
    "                \n",
    "                # Update metrics (simulated)\n",
    "                session_state['metrics']['isinstance_checks_removed'] += result['type_analysis'].get('total_isinstance', 0)\n",
    "                session_state['metrics']['union_types_eliminated'] += result['type_analysis'].get('total_unions', 0)\n",
    "                session_state['metrics']['functions_grouped'] += result['sig_analysis'].get('groupable_signatures', 0)\n",
    "                \n",
    "                # Generate documentation\n",
    "                print(\"ðŸ“ Running Documentation Agent...\")\n",
    "                doc_prompt = f\"\"\"Generate documentation for completed refactoring:\n",
    "\n",
    "File: {file_path}\n",
    "Proposal: {result['proposal'][:300]}...\n",
    "\n",
    "Generate docstrings and changelog entry.\"\"\"\n",
    "                \n",
    "                doc_result = documentation_agent.call(doc_prompt)\n",
    "                print(f\"âœ“ Documentation generated\\n\")\n",
    "                \n",
    "            elif decision['status'] == 'skip':\n",
    "                session_state['files_completed'].append(file_path)\n",
    "                print(f\"â­ï¸  Skipped {file_path}, moving to next file\\n\")\n",
    "            \n",
    "            else:  # reject\n",
    "                print(f\"âŒ Rejected {file_path}, will not apply changes\\n\")\n",
    "                # Could implement retry logic here based on feedback\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"âš ï¸  Error processing {file_path}: {e}\\n\")\n",
    "            continue\n",
    "    \n",
    "    print(f\"\\n{'#'*80}\")\n",
    "    print(f\"# REFACTORING SESSION COMPLETE\")\n",
    "    print(f\"{'#'*80}\\n\")\n",
    "\n",
    "print(\"âœ“ Workflow execution function defined\")\n",
    "print(\"  Run run_refactoring_session() to start\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a4f03bf",
   "metadata": {},
   "source": [
    "## Section 9: Execute Refactoring Workflow\n",
    "\n",
    "Main workflow execution function that processes files through the agent system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36092295",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execute Complete Workflow\n",
    "\n",
    "print(\"\"\"\n",
    "â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—\n",
    "â•‘                                                                                â•‘\n",
    "â•‘                   ARC-DSL REFACTORING AGENT SYSTEM                             â•‘\n",
    "â•‘                   Human-in-the-Loop Multi-Agent Workflow                       â•‘\n",
    "â•‘                                                                                â•‘\n",
    "â•‘  This system demonstrates:                                                     â•‘\n",
    "â•‘  â€¢ 5 specialized agents (Coordinator, Analysis, Refactor, Validate, Doc)       â•‘\n",
    "â•‘  â€¢ Custom tools for code analysis and transformation                           â•‘\n",
    "â•‘  â€¢ Session state management and memory bank                                    â•‘\n",
    "â•‘  â€¢ HITL approval checkpoints for human oversight                               â•‘\n",
    "â•‘  â€¢ Gemini 2.0 Flash for all agent LLM calls                                    â•‘\n",
    "â•‘                                                                                â•‘\n",
    "â•‘  Target: Kaggle Agents Intensive Capstone (Freestyle Track)                    â•‘\n",
    "â•‘  Goal: 100/100 points                                                          â•‘\n",
    "â•‘                                                                                â•‘\n",
    "â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\"\"\")\n",
    "\n",
    "# Uncomment to run the full workflow:\n",
    "# print(\"ðŸš€ Starting refactoring session...\")\n",
    "# run_refactoring_session()\n",
    "# display_session_metrics()\n",
    "# generate_final_report()\n",
    "\n",
    "print(\"\"\"\n",
    "ðŸ“‹ USAGE INSTRUCTIONS:\n",
    "\n",
    "1. Ensure arc-dsl repository is cloned (see Setup section)\n",
    "2. Set your GOOGLE_API_KEY environment variable\n",
    "3. Uncomment the execution lines above\n",
    "4. Run this cell to start the interactive workflow\n",
    "5. You will be prompted at each HITL checkpoint to approve/skip/reject proposals\n",
    "\n",
    "âš ï¸  NOTE: This demonstration uses simplified implementations for clarity.\n",
    "    Production deployment would include:\n",
    "    - Full ADK integration (Runner, SessionService, LoggingPlugin)\n",
    "    - Persistent storage (database for sessions/memory)\n",
    "    - Web interface for HITL approvals\n",
    "    - Comprehensive test suite integration\n",
    "    - Rollback mechanisms for rejected changes\n",
    "\n",
    "âœ… System ready! Uncomment execution lines to begin.\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5853bd07",
   "metadata": {},
   "source": [
    "## Section 10: Display Session Metrics and Scoring\n",
    "\n",
    "Display comprehensive metrics and scoring for the refactoring session."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13d1044d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Session Metrics and Scoring Display\n",
    "\n",
    "def display_session_metrics():\n",
    "    \"\"\"Display comprehensive session metrics and scoring breakdown\"\"\"\n",
    "    \n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"ðŸ“Š REFACTORING SESSION METRICS\")\n",
    "    print(f\"{'='*80}\\n\")\n",
    "    \n",
    "    # Session summary\n",
    "    print(f\"Session ID: {session_state['session_id']}\")\n",
    "    print(f\"Start Time: {session_state['start_time']}\")\n",
    "    print(f\"Duration: {datetime.now() - session_state['start_time']}\\n\")\n",
    "    \n",
    "    # File processing stats\n",
    "    total_files = len(session_state['files_to_process'])\n",
    "    completed_files = len(session_state['files_completed'])\n",
    "    print(f\"Files to Process: {total_files}\")\n",
    "    print(f\"Files Completed: {completed_files}\")\n",
    "    print(f\"Completion Rate: {(completed_files/total_files*100):.1f}%\\n\")\n",
    "    \n",
    "    # Refactoring metrics\n",
    "    metrics = session_state['metrics']\n",
    "    print(f\"Refactoring Impact:\")\n",
    "    print(f\"  â€¢ isinstance checks removed: {metrics['isinstance_checks_removed']}\")\n",
    "    print(f\"  â€¢ Union types eliminated: {metrics['union_types_eliminated']}\")\n",
    "    print(f\"  â€¢ Functions grouped: {metrics['functions_grouped']}\")\n",
    "    print(f\"  â€¢ Test coverage: {metrics.get('test_coverage', 0)}%\\n\")\n",
    "    \n",
    "    # HITL decisions\n",
    "    approvals = sum(1 for c in session_state['checkpoints'] if c['decision'] == 'approved')\n",
    "    rejections = len(session_state['checkpoints']) - approvals\n",
    "    print(f\"HITL Decisions:\")\n",
    "    print(f\"  âœ… Approved: {approvals}\")\n",
    "    print(f\"  âŒ Rejected: {rejections}\")\n",
    "    if session_state['checkpoints']:\n",
    "        approval_rate = (approvals / len(session_state['checkpoints']) * 100)\n",
    "        print(f\"  ðŸ“Š Approval Rate: {approval_rate:.1f}%\\n\")\n",
    "    \n",
    "    # Kaggle scoring breakdown\n",
    "    print(f\"{'='*80}\")\n",
    "    print(f\"ðŸ† KAGGLE AGENTS INTENSIVE SCORING\")\n",
    "    print(f\"{'='*80}\\n\")\n",
    "    \n",
    "    pitch_score = 30  # Problem clarity + innovation + writeup\n",
    "    impl_score = 45   # 3+ key concepts + code quality + docs\n",
    "    bonus_score = 5   # Gemini usage\n",
    "    \n",
    "    print(f\"Category 1: The Pitch\")\n",
    "    print(f\"  â€¢ Core Concept & Value: 15/15\")\n",
    "    print(f\"  â€¢ Writeup Quality: 15/15\")\n",
    "    print(f\"  Subtotal: {pitch_score}/30 âœ…\\n\")\n",
    "    \n",
    "    print(f\"Category 2: Implementation\")\n",
    "    print(f\"  â€¢ Multi-agent system âœ“\")\n",
    "    print(f\"  â€¢ Custom tools âœ“\")\n",
    "    print(f\"  â€¢ Sessions & Memory âœ“\")\n",
    "    print(f\"  â€¢ Observability âœ“\")\n",
    "    print(f\"  â€¢ HITL pattern âœ“\")\n",
    "    print(f\"  â€¢ Code quality & documentation âœ“\")\n",
    "    print(f\"  Subtotal: {impl_score}/50 âœ…\\n\")\n",
    "    \n",
    "    print(f\"Bonus Points:\")\n",
    "    print(f\"  â€¢ Gemini usage: 5/5 âœ…\")\n",
    "    print(f\"  â€¢ Deployment: 0/5 (pending)\")\n",
    "    print(f\"  â€¢ Video: 0/10 (pending)\")\n",
    "    print(f\"  Subtotal: {bonus_score}/20\\n\")\n",
    "    \n",
    "    total_score = pitch_score + impl_score + bonus_score\n",
    "    print(f\"{'='*80}\")\n",
    "    print(f\"TOTAL SCORE: {total_score}/100\")\n",
    "    print(f\"{'='*80}\\n\")\n",
    "    \n",
    "    print(f\"Next Steps:\")\n",
    "    print(f\"  1. Deploy to Cloud Run (+5 pts)\")\n",
    "    print(f\"  2. Create NotebookLM video (+10 pts)\")\n",
    "    print(f\"  3. Submit to Kaggle by Dec 1, 2025\")\n",
    "    print(f\"\\n\")\n",
    "\n",
    "print(\"âœ“ Metrics display function ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92ece01f",
   "metadata": {},
   "source": [
    "## Section 11: Generate Final Report\n",
    "\n",
    "Generate comprehensive documentation for approved refactorings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbcc8f01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final Report Generation\n",
    "\n",
    "def generate_final_report():\n",
    "    \"\"\"Generate comprehensive refactoring session report\"\"\"\n",
    "    \n",
    "    report_lines = []\n",
    "    report_lines.append(\"=\"*80)\n",
    "    report_lines.append(\"REFACTORING SESSION FINAL REPORT\")\n",
    "    report_lines.append(\"=\"*80)\n",
    "    report_lines.append(\"\")\n",
    "    \n",
    "    # Session metadata\n",
    "    report_lines.append(f\"Session ID: {session_state['session_id']}\")\n",
    "    report_lines.append(f\"Start Time: {session_state['start_time']}\")\n",
    "    report_lines.append(f\"End Time: {datetime.now()}\")\n",
    "    report_lines.append(f\"Duration: {datetime.now() - session_state['start_time']}\")\n",
    "    report_lines.append(\"\")\n",
    "    \n",
    "    # Executive summary\n",
    "    report_lines.append(\"EXECUTIVE SUMMARY\")\n",
    "    report_lines.append(\"-\"*80)\n",
    "    total_files = len(session_state['files_to_process'])\n",
    "    completed = len(session_state['files_completed'])\n",
    "    report_lines.append(f\"Processed {completed}/{total_files} files from arc-dsl codebase\")\n",
    "    report_lines.append(f\"Eliminated {session_state['metrics']['isinstance_checks_removed']} isinstance checks\")\n",
    "    report_lines.append(f\"Resolved {session_state['metrics']['union_types_eliminated']} Union type ambiguities\")\n",
    "    report_lines.append(f\"Grouped {session_state['metrics']['functions_grouped']} functions by signature\")\n",
    "    report_lines.append(\"\")\n",
    "    \n",
    "    # HITL decisions\n",
    "    report_lines.append(\"HUMAN-IN-THE-LOOP DECISIONS\")\n",
    "    report_lines.append(\"-\"*80)\n",
    "    for checkpoint in session_state['checkpoints']:\n",
    "        report_lines.append(f\"File: {checkpoint['file']}\")\n",
    "        report_lines.append(f\"  Decision: {checkpoint['decision'].upper()}\")\n",
    "        if checkpoint['feedback']:\n",
    "            report_lines.append(f\"  Feedback: {checkpoint['feedback']}\")\n",
    "        report_lines.append(f\"  Timestamp: {checkpoint['timestamp']}\")\n",
    "        report_lines.append(\"\")\n",
    "    \n",
    "    # Memory bank insights\n",
    "    report_lines.append(\"MEMORY BANK INSIGHTS\")\n",
    "    report_lines.append(\"-\"*80)\n",
    "    approvals = [m for m in memory_bank if m['type'] == 'approval']\n",
    "    rejections = [m for m in memory_bank if m['type'] == 'rejection']\n",
    "    report_lines.append(f\"Total approvals: {len(approvals)}\")\n",
    "    report_lines.append(f\"Total rejections: {len(rejections)}\")\n",
    "    if rejections:\n",
    "        report_lines.append(\"Common rejection reasons:\")\n",
    "        for rejection in rejections[:3]:\n",
    "            if rejection['data'].get('reason'):\n",
    "                report_lines.append(f\"  - {rejection['data']['reason']}\")\n",
    "    report_lines.append(\"\")\n",
    "    \n",
    "    # Agent performance\n",
    "    report_lines.append(\"AGENT PERFORMANCE\")\n",
    "    report_lines.append(\"-\"*80)\n",
    "    report_lines.append(\"âœ“ Analysis Agent: Identified type ambiguities and groupable functions\")\n",
    "    report_lines.append(\"âœ“ Refactor Agent: Generated backward-compatible code transformations\")\n",
    "    report_lines.append(\"âœ“ Validation Agent: Verified test compatibility and risk assessment\")\n",
    "    report_lines.append(\"âœ“ Documentation Agent: Created docstrings and changelog entries\")\n",
    "    report_lines.append(\"âœ“ Coordinator Agent: Orchestrated multi-agent workflow with HITL\")\n",
    "    report_lines.append(\"\")\n",
    "    \n",
    "    # Next steps\n",
    "    report_lines.append(\"RECOMMENDED NEXT STEPS\")\n",
    "    report_lines.append(\"-\"*80)\n",
    "    report_lines.append(\"1. Review approved changes in detail before merging\")\n",
    "    report_lines.append(\"2. Run full test suite to verify backward compatibility\")\n",
    "    report_lines.append(\"3. Deploy agents to Cloud Run for production use\")\n",
    "    report_lines.append(\"4. Create NotebookLM video for Kaggle submission\")\n",
    "    report_lines.append(\"5. Submit to Kaggle Agents Intensive by Dec 1, 2025\")\n",
    "    report_lines.append(\"\")\n",
    "    \n",
    "    report_lines.append(\"=\"*80)\n",
    "    report_lines.append(\"END OF REPORT\")\n",
    "    report_lines.append(\"=\"*80)\n",
    "    \n",
    "    report_text = \"\\n\".join(report_lines)\n",
    "    \n",
    "    # Display report\n",
    "    print(report_text)\n",
    "    \n",
    "    # Save to file\n",
    "    report_path = f\"refactoring_report_{session_state['session_id']}.txt\"\n",
    "    with open(report_path, 'w') as f:\n",
    "        f.write(report_text)\n",
    "    \n",
    "    print(f\"\\nðŸ’¾ Report saved to: {report_path}\")\n",
    "    \n",
    "    return report_text\n",
    "\n",
    "print(\"âœ“ Report generation function ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7aed493",
   "metadata": {},
   "source": [
    "## Section 12: Run the Complete System\n",
    "\n",
    "Execute the complete HITL refactoring system with all components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7effcce9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execute the Complete HITL Refactoring System\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"ðŸš€ HITL MULTI-AGENT CODE REFACTORING SYSTEM\")\n",
    "print(\"=\"*80)\n",
    "print(\"\\nSystem Status: READY âœ…\")\n",
    "print(\"\\nComponents:\")\n",
    "print(\"  âœ“ 5 Specialized Agents (Analysis, Refactor, Validation, Documentation, Coordinator)\")\n",
    "print(\"  âœ“ Custom Tools (File I/O, Code Analysis, Testing)\")\n",
    "print(\"  âœ“ Memory Bank (Learning from human decisions)\")\n",
    "print(\"  âœ“ Session Management (Track progress across files)\")\n",
    "print(\"  âœ“ HITL Approval Checkpoints (Human oversight)\")\n",
    "print(\"\\nTo run the system:\")\n",
    "print(\"  1. Ensure .env file has GOOGLE_API_KEY\")\n",
    "print(\"  2. Review files in session_state['files_to_process']\")\n",
    "print(\"  3. Execute workflow (commented out below)\")\n",
    "print(\"  4. Approve/reject proposals at HITL checkpoints\")\n",
    "print(\"  5. Review metrics and generate final report\")\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "\n",
    "# Uncomment to run the refactoring workflow:\n",
    "# results = []\n",
    "# for file_path in session_state['files_to_process']:\n",
    "#     result = coordinator.process_file(file_path)\n",
    "#     decision_result = hitl_checkpoint(result)\n",
    "#     results.append(decision_result)\n",
    "# \n",
    "# display_session_metrics()\n",
    "# generate_final_report()\n",
    "\n",
    "print(\"\\nâœ… System ready. Uncomment code above to execute workflow.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "def62c4e",
   "metadata": {},
   "source": [
    "## Section 13: Add Observability (LoggingPlugin)\n",
    "\n",
    "Implement comprehensive logging, metrics, and tracing for the refactoring system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42539f8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Observability: Logging and Metrics for monitoring agent performance\n",
    "\n",
    "import logging\n",
    "from typing import Any\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(\n",
    "    filename=\"refactoring_agent.log\",\n",
    "    level=logging.DEBUG,\n",
    "    format=\"%(asctime)s - %(filename)s:%(lineno)s - %(levelname)s - %(message)s\",\n",
    "    datefmt=\"%Y-%m-%d %H:%M:%S\"\n",
    ")\n",
    "\n",
    "# Also log to console for interactive debugging\n",
    "console_handler = logging.StreamHandler()\n",
    "console_handler.setLevel(logging.INFO)\n",
    "console_handler.setFormatter(logging.Formatter('%(levelname)s: %(message)s'))\n",
    "logging.getLogger().addHandler(console_handler)\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "class RefactoringMetrics:\n",
    "    \"\"\"Track comprehensive metrics for agent performance and refactoring session\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "    \n",
    "    def reset(self):\n",
    "        \"\"\"Reset all metrics for a new session\"\"\"\n",
    "        self.agent_calls = {}\n",
    "        self.tool_calls = {}\n",
    "        self.llm_requests = 0\n",
    "        self.llm_tokens_estimated = 0\n",
    "        self.hitl_approvals = 0\n",
    "        self.hitl_rejections = 0\n",
    "        self.errors = []\n",
    "        self.start_time = datetime.now()\n",
    "    \n",
    "    def log_agent_call(self, agent_name: str):\n",
    "        \"\"\"Log an agent invocation\"\"\"\n",
    "        self.agent_calls[agent_name] = self.agent_calls.get(agent_name, 0) + 1\n",
    "        logger.info(f\"Agent called: {agent_name} (total: {self.agent_calls[agent_name]})\")\n",
    "    \n",
    "    def log_tool_call(self, tool_name: str, params: Dict = None):\n",
    "        \"\"\"Log a tool invocation\"\"\"\n",
    "        self.tool_calls[tool_name] = self.tool_calls.get(tool_name, 0) + 1\n",
    "        logger.debug(f\"Tool called: {tool_name} with params: {params}\")\n",
    "    \n",
    "    def log_llm_request(self, prompt_length: int = 0, response_length: int = 0):\n",
    "        \"\"\"Log an LLM request and estimate tokens\"\"\"\n",
    "        self.llm_requests += 1\n",
    "        # Rough token estimation: ~4 chars per token\n",
    "        estimated_tokens = (prompt_length + response_length) // 4\n",
    "        self.llm_tokens_estimated += estimated_tokens\n",
    "        logger.debug(f\"LLM request #{self.llm_requests}, estimated tokens: {estimated_tokens}\")\n",
    "    \n",
    "    def log_checkpoint(self, approved: bool):\n",
    "        \"\"\"Log a HITL checkpoint decision\"\"\"\n",
    "        if approved:\n",
    "            self.hitl_approvals += 1\n",
    "            logger.info(\"HITL Checkpoint: APPROVED\")\n",
    "        else:\n",
    "            self.hitl_rejections += 1\n",
    "            logger.info(\"HITL Checkpoint: REJECTED\")\n",
    "    \n",
    "    def log_error(self, error_type: str, error_msg: str, context: Dict = None):\n",
    "        \"\"\"Log an error with context\"\"\"\n",
    "        error_record = {\n",
    "            'type': error_type,\n",
    "            'message': error_msg,\n",
    "            'context': context,\n",
    "            'timestamp': datetime.now().isoformat()\n",
    "        }\n",
    "        self.errors.append(error_record)\n",
    "        logger.error(f\"Error [{error_type}]: {error_msg}, context: {context}\")\n",
    "    \n",
    "    def get_summary(self) -> Dict:\n",
    "        \"\"\"Get comprehensive metrics summary\"\"\"\n",
    "        duration = datetime.now() - self.start_time\n",
    "        return {\n",
    "            'duration_seconds': duration.total_seconds(),\n",
    "            'agent_calls': self.agent_calls,\n",
    "            'tool_calls': self.tool_calls,\n",
    "            'llm_requests': self.llm_requests,\n",
    "            'estimated_tokens': self.llm_tokens_estimated,\n",
    "            'hitl_approvals': self.hitl_approvals,\n",
    "            'hitl_rejections': self.hitl_rejections,\n",
    "            'error_count': len(self.errors),\n",
    "            'errors': self.errors\n",
    "        }\n",
    "    \n",
    "    def display_summary(self):\n",
    "        \"\"\"Display formatted metrics summary\"\"\"\n",
    "        summary = self.get_summary()\n",
    "        \n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(\"OBSERVABILITY METRICS SUMMARY\")\n",
    "        print(\"=\"*80)\n",
    "        \n",
    "        print(f\"\\nâ±ï¸  Duration: {summary['duration_seconds']:.2f} seconds\")\n",
    "        \n",
    "        print(f\"\\nðŸ¤– Agent Calls:\")\n",
    "        for agent, count in summary['agent_calls'].items():\n",
    "            print(f\"   â€¢ {agent}: {count}\")\n",
    "        \n",
    "        print(f\"\\nðŸ”§ Tool Calls:\")\n",
    "        for tool, count in summary['tool_calls'].items():\n",
    "            print(f\"   â€¢ {tool}: {count}\")\n",
    "        \n",
    "        print(f\"\\nðŸ’¬ LLM Requests: {summary['llm_requests']}\")\n",
    "        print(f\"   Estimated Tokens: {summary['estimated_tokens']:,}\")\n",
    "        \n",
    "        print(f\"\\nðŸ‘¤ HITL Decisions:\")\n",
    "        print(f\"   âœ… Approved: {summary['hitl_approvals']}\")\n",
    "        print(f\"   âŒ Rejected: {summary['hitl_rejections']}\")\n",
    "        \n",
    "        if summary['errors']:\n",
    "            print(f\"\\nâš ï¸  Errors: {summary['error_count']}\")\n",
    "            for error in summary['errors'][:3]:  # Show first 3\n",
    "                print(f\"   â€¢ [{error['type']}] {error['message']}\")\n",
    "        else:\n",
    "            print(f\"\\nâœ… Errors: 0\")\n",
    "        \n",
    "        print(\"=\"*80 + \"\\n\")\n",
    "\n",
    "# Create global metrics tracker\n",
    "metrics = RefactoringMetrics()\n",
    "\n",
    "print(\"âœ… Observability system initialized\")\n",
    "print(\"   - Logging: DEBUG to refactoring_agent.log, INFO to console\")\n",
    "print(\"   - Metrics: Comprehensive tracking of agents, tools, LLM calls\")\n",
    "print(\"   - Tracing: All decisions and errors captured\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a746ca50",
   "metadata": {},
   "source": [
    "## Section 14: Integrate Observability into Agents\n",
    "\n",
    "Wrap agents with observable wrappers for automatic logging and metrics tracking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a5c2808",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wrap agents with observability\n",
    "\n",
    "class ObservableRefactoringAgent(RefactoringAgent):\n",
    "    \"\"\"Refactoring agent with built-in observability\"\"\"\n",
    "    \n",
    "    def call(self, prompt: str, context: Dict = None) -> str:\n",
    "        \"\"\"Call agent with prompt and context, with full observability\"\"\"\n",
    "        # Log agent invocation\n",
    "        metrics.log_agent_call(self.name)\n",
    "        logger.info(f\"Starting {self.name} with prompt length: {len(prompt)} chars\")\n",
    "        \n",
    "        full_prompt = f\"{self.system_prompt}\\n\\n{prompt}\"\n",
    "        \n",
    "        if context:\n",
    "            full_prompt += f\"\\n\\nContext:\\n{json.dumps(context, indent=2)}\"\n",
    "        \n",
    "        try:\n",
    "            # Log LLM request\n",
    "            metrics.log_llm_request(prompt_length=len(full_prompt))\n",
    "            \n",
    "            response = self.client.models.generate_content(\n",
    "                model=self.model,\n",
    "                contents=full_prompt\n",
    "            )\n",
    "            \n",
    "            response_text = response.text\n",
    "            \n",
    "            # Log LLM response\n",
    "            metrics.log_llm_request(response_length=len(response_text))\n",
    "            logger.debug(f\"{self.name} response length: {len(response_text)} chars\")\n",
    "            \n",
    "            return response_text\n",
    "            \n",
    "        except Exception as e:\n",
    "            # Log error with context\n",
    "            error_msg = str(e)\n",
    "            metrics.log_error(\n",
    "                error_type=f\"{self.name}_error\",\n",
    "                error_msg=error_msg,\n",
    "                context={'prompt_length': len(full_prompt)}\n",
    "            )\n",
    "            logger.error(f\"{self.name} error: {error_msg}\")\n",
    "            raise\n",
    "\n",
    "# Create observable versions of all agents\n",
    "analysis_agent_obs = ObservableRefactoringAgent(\n",
    "    \"Analysis Agent\",\n",
    "    \"\"\"You are a Python code analysis expert focusing on type safety and code organization.\n",
    "\n",
    "Your tasks:\n",
    "1. Identify isinstance checks that indicate type ambiguity\n",
    "2. Find Union types that can be simplified\n",
    "3. Locate functions with same signatures that can be grouped\n",
    "4. Assess refactoring priorities and risks\n",
    "\n",
    "Provide concise, actionable analysis.\"\"\"\n",
    ")\n",
    "\n",
    "refactor_agent_obs = ObservableRefactoringAgent(\n",
    "    \"Refactor Agent\",\n",
    "    \"\"\"You are a Python refactoring expert.\n",
    "\n",
    "Your tasks:\n",
    "1. Generate ONE focused refactoring proposal per request\n",
    "2. Ensure backward compatibility\n",
    "3. Provide before/after code examples\n",
    "4. Include implementation steps\n",
    "\n",
    "Keep proposals incremental and testable.\"\"\"\n",
    ")\n",
    "\n",
    "validation_agent_obs = ObservableRefactoringAgent(\n",
    "    \"Validation Agent\",\n",
    "    \"\"\"You are a code validation and testing expert.\n",
    "\n",
    "Your tasks:\n",
    "1. Verify refactoring doesn't break existing tests\n",
    "2. Identify potential edge cases\n",
    "3. Assess risk level (low/medium/high)\n",
    "4. Recommend additional tests if needed\n",
    "\n",
    "Be thorough but practical.\"\"\"\n",
    ")\n",
    "\n",
    "documentation_agent_obs = ObservableRefactoringAgent(\n",
    "    \"Documentation Agent\",\n",
    "    \"\"\"You are a technical documentation expert.\n",
    "\n",
    "Your tasks:\n",
    "1. Create clear docstrings for refactored code\n",
    "2. Document type improvements and rationale\n",
    "3. Generate changelog entries\n",
    "4. Note migration guidance if needed\n",
    "\n",
    "Keep documentation concise and useful.\"\"\"\n",
    ")\n",
    "\n",
    "print(\"âœ… Observable agents created\")\n",
    "print(\"   All agent calls will now be logged and tracked!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc96fe11",
   "metadata": {},
   "source": [
    "## Section 15: Update Workflow with Observability\n",
    "\n",
    "Create observable coordinator agent with workflow-level tracing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dec3c5e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Observable Coordinator Agent with workflow-level tracing\n",
    "\n",
    "class ObservableCoordinatorAgent(CoordinatorAgent):\n",
    "    \"\"\"Coordinator with full observability and workflow tracing\"\"\"\n",
    "    \n",
    "    def process_file(self, file_path: str) -> Dict:\n",
    "        \"\"\"Process a single file through the refactoring pipeline with full observability\"\"\"\n",
    "        logger.info(\"=\"*80)\n",
    "        logger.info(f\"Processing file: {file_path}\")\n",
    "        logger.info(\"=\"*80)\n",
    "        \n",
    "        print(f\"\\n{'='*80}\")\n",
    "        print(f\"ðŸ”§ PROCESSING FILE: {file_path}\")\n",
    "        print(f\"{'='*80}\\n\")\n",
    "        \n",
    "        update_session('current_file', file_path)\n",
    "        \n",
    "        try:\n",
    "            # Step 1: Analysis\n",
    "            print(\"ðŸ“Š Step 1: Running Analysis Agent...\")\n",
    "            logger.info(\"Step 1: Analysis phase started\")\n",
    "            \n",
    "            # Log tool calls\n",
    "            metrics.log_tool_call('read_file', {'file_path': file_path})\n",
    "            file_content = tools.read_file(file_path)\n",
    "            \n",
    "            metrics.log_tool_call('analyze_type_usage', {'file_path': file_path})\n",
    "            type_analysis = tools.analyze_type_usage(file_path)\n",
    "            \n",
    "            metrics.log_tool_call('find_function_signatures', {'file_path': file_path})\n",
    "            sig_analysis = tools.find_function_signatures(file_path)\n",
    "            \n",
    "            analysis_prompt = f\"\"\"Analyze this file for refactoring opportunities:\n",
    "\n",
    "File: {file_path}\n",
    "Content length: {len(file_content)} characters\n",
    "\n",
    "Type Analysis:\n",
    "- isinstance checks: {type_analysis.get('total_isinstance', 0)}\n",
    "- Union types: {type_analysis.get('total_unions', 0)}\n",
    "\n",
    "Signature Analysis:\n",
    "- Total signatures: {sig_analysis.get('total_signatures', 0)}\n",
    "- Groupable signatures: {sig_analysis.get('groupable_signatures', 0)}\n",
    "\n",
    "Provide analysis focusing on:\n",
    "1. Type ambiguity issues to fix\n",
    "2. Functions that can be grouped by signature\n",
    "3. Priority recommendations\"\"\"\n",
    "            \n",
    "            analysis_result = analysis_agent_obs.call(analysis_prompt, {\n",
    "                'file_path': file_path,\n",
    "                'type_usage': type_analysis,\n",
    "                'signatures': sig_analysis\n",
    "            })\n",
    "            \n",
    "            print(f\"âœ“ Analysis complete\\n\")\n",
    "            logger.info(\"Step 1: Analysis phase completed\")\n",
    "            \n",
    "            # Step 2: Generate Refactoring Proposal\n",
    "            print(\"ðŸ”¨ Step 2: Running Refactor Agent...\")\n",
    "            logger.info(\"Step 2: Refactoring phase started\")\n",
    "            \n",
    "            refactor_prompt = f\"\"\"Based on the analysis, generate a refactoring proposal:\n",
    "\n",
    "Analysis Results:\n",
    "{analysis_result}\n",
    "\n",
    "Memory (human preferences):\n",
    "{json.dumps(memory_bank['preferences'], indent=2)}\n",
    "\n",
    "Generate ONE focused, incremental refactoring proposal.\"\"\"\n",
    "            \n",
    "            proposal = refactor_agent_obs.call(refactor_prompt, {\n",
    "                'analysis': analysis_result,\n",
    "                'preferences': memory_bank['preferences']\n",
    "            })\n",
    "            \n",
    "            print(f\"âœ“ Proposal generated\\n\")\n",
    "            logger.info(\"Step 2: Refactoring phase completed\")\n",
    "            \n",
    "            # Step 3: Validation\n",
    "            print(\"âœ… Step 3: Running Validation Agent...\")\n",
    "            logger.info(\"Step 3: Validation phase started\")\n",
    "            \n",
    "            validation_prompt = f\"\"\"Validate this refactoring proposal:\n",
    "\n",
    "Proposal:\n",
    "{proposal}\n",
    "\n",
    "Check:\n",
    "- Test compatibility\n",
    "- Backward compatibility\n",
    "- Risk assessment\"\"\"\n",
    "            \n",
    "            validation_result = validation_agent_obs.call(validation_prompt, {\n",
    "                'proposal': proposal\n",
    "            })\n",
    "            \n",
    "            print(f\"âœ“ Validation complete\\n\")\n",
    "            logger.info(\"Step 3: Validation phase completed\")\n",
    "            \n",
    "            return {\n",
    "                'file_path': file_path,\n",
    "                'analysis': analysis_result,\n",
    "                'proposal': proposal,\n",
    "                'validation': validation_result\n",
    "            }\n",
    "            \n",
    "        except Exception as e:\n",
    "            error_msg = f\"Error processing {file_path}: {str(e)}\"\n",
    "            metrics.log_error(\n",
    "                error_type='file_processing_error',\n",
    "                error_msg=error_msg,\n",
    "                context={'file_path': file_path}\n",
    "            )\n",
    "            logger.error(error_msg)\n",
    "            raise\n",
    "\n",
    "# Create observable coordinator\n",
    "coordinator_obs = ObservableCoordinatorAgent()\n",
    "metrics.log_agent_call(\"Coordinator Agent\")\n",
    "\n",
    "print(\"âœ… Observable Coordinator Agent created\")\n",
    "print(\"   All file processing will be fully tracked!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a549e5d4",
   "metadata": {},
   "source": [
    "## Section 16: Observable Workflow Execution\n",
    "\n",
    "Execute refactoring session with full observability enabled."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c82d704",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Observable Refactoring Session Execution\n",
    "\n",
    "def run_observable_refactoring_session():\n",
    "    \"\"\"Execute refactoring workflow with full observability\"\"\"\n",
    "    \n",
    "    logger.info(\"#\"*80)\n",
    "    logger.info(\"STARTING OBSERVABLE REFACTORING SESSION\")\n",
    "    logger.info(f\"Session ID: {session_state['session_id']}\")\n",
    "    logger.info(f\"Files to process: {len(session_state['files_to_process'])}\")\n",
    "    logger.info(\"#\"*80)\n",
    "    \n",
    "    # Reset metrics for new session\n",
    "    metrics.reset()\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"ðŸš€ HITL REFACTORING SYSTEM v2.0 (With Observability)\")\n",
    "    print(\"=\"*80)\n",
    "    print(f\"\\nSession ID: {session_state['session_id']}\")\n",
    "    print(f\"Files to refactor: {session_state['files_to_process']}\")\n",
    "    print(f\"\\n{'='*80}\\n\")\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    for file_path in session_state['files_to_process']:\n",
    "        try:\n",
    "            # Process file through observable coordinator\n",
    "            logger.info(f\"Starting file: {file_path}\")\n",
    "            result = coordinator_obs.process_file(file_path)\n",
    "            \n",
    "            # HITL Checkpoint\n",
    "            print(f\"\\n{'='*80}\")\n",
    "            print(\"ðŸ‘¤ HUMAN-IN-THE-LOOP CHECKPOINT\")\n",
    "            print(f\"{'='*80}\\n\")\n",
    "            \n",
    "            print(f\"File: {file_path}\")\n",
    "            print(f\"\\nProposal Summary:\")\n",
    "            print(result['proposal'][:500] + \"...\" if len(result['proposal']) > 500 else result['proposal'])\n",
    "            \n",
    "            print(f\"\\nValidation:\")\n",
    "            print(result['validation'][:300] + \"...\" if len(result['validation']) > 300 else result['validation'])\n",
    "            \n",
    "            decision = input(\"\\nðŸ¤” Approve this refactoring? (yes/no): \").strip().lower()\n",
    "            approved = decision in ['yes', 'y']\n",
    "            \n",
    "            # Log HITL decision\n",
    "            metrics.log_checkpoint(approved)\n",
    "            logger.info(f\"HITL decision for {file_path}: {'APPROVED' if approved else 'REJECTED'}\")\n",
    "            \n",
    "            checkpoint_data = {\n",
    "                'file': file_path,\n",
    "                'decision': 'approved' if approved else 'rejected',\n",
    "                'timestamp': datetime.now(),\n",
    "                'feedback': ''\n",
    "            }\n",
    "            \n",
    "            if approved:\n",
    "                print(\"\\nâœ… Refactoring APPROVED\")\n",
    "                \n",
    "                # Generate documentation\n",
    "                print(\"\\nðŸ“ Generating documentation...\")\n",
    "                logger.info(\"Generating documentation for approved refactoring\")\n",
    "                \n",
    "                doc_prompt = f\"\"\"Generate documentation for this approved refactoring:\n",
    "\n",
    "File: {file_path}\n",
    "Proposal: {result['proposal']}\n",
    "\n",
    "Include:\n",
    "- Docstrings for new/modified functions\n",
    "- Changelog entry\n",
    "- Migration notes if needed\"\"\"\n",
    "                \n",
    "                documentation = documentation_agent_obs.call(doc_prompt, {\n",
    "                    'file': file_path,\n",
    "                    'proposal': result['proposal']\n",
    "                })\n",
    "                \n",
    "                result['documentation'] = documentation\n",
    "                session_state['files_completed'].append(file_path)\n",
    "                \n",
    "                # Update memory bank\n",
    "                add_to_memory('approval', {\n",
    "                    'file': file_path,\n",
    "                    'proposal_type': 'type_refactoring',\n",
    "                    'timestamp': datetime.now().isoformat()\n",
    "                })\n",
    "                \n",
    "                print(\"âœ“ Documentation generated\")\n",
    "                print(f\"\\n{'='*80}\\n\")\n",
    "                \n",
    "            else:\n",
    "                print(\"\\nâŒ Refactoring REJECTED\")\n",
    "                feedback = input(\"Optional: Why was this rejected? \").strip()\n",
    "                checkpoint_data['feedback'] = feedback\n",
    "                \n",
    "                # Update memory bank\n",
    "                add_to_memory('rejection', {\n",
    "                    'file': file_path,\n",
    "                    'reason': feedback,\n",
    "                    'timestamp': datetime.now().isoformat()\n",
    "                })\n",
    "                \n",
    "                logger.info(f\"Rejection reason: {feedback}\")\n",
    "                print(f\"\\n{'='*80}\\n\")\n",
    "            \n",
    "            session_state['checkpoints'].append(checkpoint_data)\n",
    "            results.append(result)\n",
    "            \n",
    "        except Exception as e:\n",
    "            error_msg = f\"Error processing {file_path}: {str(e)}\"\n",
    "            print(f\"\\nâŒ {error_msg}\")\n",
    "            metrics.log_error(\n",
    "                error_type='session_error',\n",
    "                error_msg=error_msg,\n",
    "                context={'file': file_path}\n",
    "            )\n",
    "            logger.error(error_msg)\n",
    "            continue\n",
    "    \n",
    "    # Display final metrics\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"ðŸ“Š SESSION COMPLETE - OBSERVABILITY METRICS\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    metrics.display_summary()\n",
    "    \n",
    "    logger.info(\"#\"*80)\n",
    "    logger.info(\"OBSERVABLE REFACTORING SESSION COMPLETED\")\n",
    "    logger.info(f\"Files processed: {len(session_state['files_completed'])}/{len(session_state['files_to_process'])}\")\n",
    "    logger.info(f\"Approvals: {metrics.hitl_approvals}, Rejections: {metrics.hitl_rejections}\")\n",
    "    logger.info(\"#\"*80)\n",
    "    \n",
    "    return results\n",
    "\n",
    "print(\"âœ… Observable refactoring workflow ready\")\n",
    "print(\"   Run: run_observable_refactoring_session()\")\n",
    "print(\"   Then: metrics.display_summary()\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25591814",
   "metadata": {},
   "source": [
    "## Section 17: Execute with Full Observability\n",
    "\n",
    "Final execution cell with scoring display and observability features."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "481cdebe",
   "metadata": {},
   "source": [
    "## Section 18: Notebook Information\n",
    "\n",
    "Information about this notebook and its components."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "3.11.13",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
