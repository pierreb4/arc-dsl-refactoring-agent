{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ba2755ca",
   "metadata": {},
   "source": [
    "## ðŸš€ Quick Start Guide\n",
    "\n",
    "**Before running this notebook:**\n",
    "\n",
    "1. **Get a Gemini API Key**\n",
    "   - Visit [Google AI Studio](https://aistudio.google.com/app/api-keys)\n",
    "   - Click \"Create API Key\"\n",
    "   - Copy your API key\n",
    "\n",
    "2. **Create `.env` file**\n",
    "   - In this directory (`/code/`), create a file named `.env`\n",
    "   - Add the following line:\n",
    "   ```\n",
    "   GOOGLE_API_KEY=your_actual_api_key_here\n",
    "   ```\n",
    "   - Save the file\n",
    "\n",
    "3. **Run the notebook**\n",
    "   - Execute cells in order from top to bottom\n",
    "   - The system will load your API key automatically\n",
    "   - Interactive HITL checkpoints will prompt for approval/rejection\n",
    "   - MCP Python Refactoring provides professional-grade analysis\n",
    "\n",
    "\n",
    "**Note:** A `.env.example` file is provided as a template.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2e5860d",
   "metadata": {},
   "source": [
    "# ARC-DSL Type Annotation Agent - Phase 1\n",
    "\n",
    "**Project:** ARC-DSL Refactoring Agent System  \n",
    "**Track:** Kaggle Agents Intensive - Freestyle  \n",
    "**Date:** November 24, 2025\n",
    "\n",
    "## Overview\n",
    "\n",
    "A human-in-the-loop (HITL) agent system that incrementally adds type annotations to the [arc-dsl codebase](https://github.com/michaelhodel/arc-dsl) through intelligent analysis and validation.\n",
    "\n",
    "**Core Philosophy:** Humans approve strategy, agents execute tactics.\n",
    "\n",
    "### Phase 1: Type Annotation System\n",
    "\n",
    "Phase 1 focuses on adding type hints to 400+ solver functions in `solvers.py` using the strict ARC-DSL type system (Grid, Piece, Object, Objects, Indices, Integer, IntegerSet).\n",
    "\n",
    "**Key Features:**\n",
    "- **Specialized Agent:** Type annotation expert with domain-specific knowledge\n",
    "- **Automated Analysis:** External analyzer script (`analyze_solver_types.py`) infers types\n",
    "- **HITL Workflow:** Interactive approval with refine/skip/abort options\n",
    "- **Automatic Testing:** Runs `tests.py` and `main.py` after each approval\n",
    "- **Auto-Rollback:** Restores original code if tests fail (regression prevention)\n",
    "- **Progress Tracking:** Shows annotation progress across the codebase\n",
    "\n",
    "### Current Status\n",
    "\n",
    "- **Implementation:** âœ… Complete and tested\n",
    "- **Solvers Processed:** 10 test cases (8 approved, 1 refined, 2 skipped)\n",
    "- **Test Results:** All regression tests passing\n",
    "- **Next Steps:** Scale to full 400+ solver functions\n",
    "\n",
    "### Refactoring Goals\n",
    "\n",
    "1. **Add Type Annotations:** Eliminate type ambiguity with explicit type hints\n",
    "2. **Maintain Compatibility:** Zero regressions (tests.py must pass, main.py â‰¥390/1000)\n",
    "3. **Incremental Progress:** One function at a time with HITL validation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73613729",
   "metadata": {},
   "source": [
    "## Section 1: Import Libraries\n",
    "\n",
    "Import all necessary libraries for the multi-agent system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7afc40a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n",
      "âœ“ arc-dsl repository already exists\n"
     ]
    }
   ],
   "source": [
    "# Install required packages\n",
    "%pip install -q python-dotenv google-genai google-adk ipywidgets\n",
    "\n",
    "# Clone arc-dsl repository if not already present\n",
    "import os\n",
    "if not os.path.exists('arc-dsl'):\n",
    "    !git clone https://github.com/michaelhodel/arc-dsl.git\n",
    "    print(\"âœ“ arc-dsl repository cloned\")\n",
    "else:\n",
    "    print(\"âœ“ arc-dsl repository already exists\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df1b4a72",
   "metadata": {},
   "source": [
    "## Section 2: Configure Gemini API Key\n",
    "\n",
    "Load the Gemini API key from the .env file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6b8abb43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Gemini API key setup complete.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "try:\n",
    "    # Load environment variables from .env file\n",
    "    load_dotenv()\n",
    "    GOOGLE_API_KEY = os.getenv(\"GOOGLE_API_KEY\")\n",
    "    os.environ[\"GOOGLE_API_KEY\"] = GOOGLE_API_KEY\n",
    "    print(\"âœ… Gemini API key setup complete.\")\n",
    "except Exception as e:\n",
    "    print(\n",
    "        f\"ðŸ”‘ Authentication Error: Please make sure you have added 'GOOGLE_API_KEY' to your .env file. Details: {e}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9d9aa12a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Gemini client configured\n",
      "   Model: gemini-2.0-flash-exp\n",
      "   Retry configuration: 5 attempts\n",
      "   Exponential backoff: 1s â†’ 7s â†’ 49s â†’ 343s â†’ 2401s\n",
      "   Retry on HTTP: [429, 500, 503, 504]\n"
     ]
    }
   ],
   "source": [
    "# Configure Gemini API\n",
    "from google import genai\n",
    "from google.genai import types\n",
    "\n",
    "MODEL_NAME = 'gemini-2.0-flash-exp'  # Using Gemini 2.0 Flash\n",
    "\n",
    "# Initialize Gemini client\n",
    "client = genai.Client(api_key=os.environ.get(\"GOOGLE_API_KEY\"))\n",
    "\n",
    "# Configure retry options for handling transient errors (429, 500, 503, 504)\n",
    "retry_config = types.HttpRetryOptions(\n",
    "    attempts=5,  # Maximum retry attempts\n",
    "    exp_base=7,  # Exponential backoff multiplier (delay grows: 1s, 7s, 49s...)\n",
    "    initial_delay=1,  # Initial delay before first retry (in seconds)\n",
    "    http_status_codes=[429, 500, 503, 504],  # Retry on these HTTP errors\n",
    ")\n",
    "\n",
    "print(\"âœ… Gemini client configured\")\n",
    "print(f\"   Model: {MODEL_NAME}\")\n",
    "print(f\"   Retry configuration: {retry_config.attempts} attempts\")\n",
    "print(f\"   Exponential backoff: 1s â†’ 7s â†’ 49s â†’ 343s â†’ 2401s\")\n",
    "print(f\"   Retry on HTTP: {retry_config.http_status_codes}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fe59afb",
   "metadata": {},
   "source": [
    "## Section 3: Configure Gemini Client\n",
    "\n",
    "Initialize the Gemini client with retry configuration for the agent system."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea63276d",
   "metadata": {},
   "source": [
    "## Section 4: Agent Base Class\n",
    "\n",
    "Define the base RefactoringAgent class used by the type annotation system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "91dcc396",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ RefactoringAgent base class defined\n"
     ]
    }
   ],
   "source": [
    "# Agent Base Class Implementation\n",
    "import time\n",
    "from typing import Dict\n",
    "\n",
    "class RefactoringAgent:\n",
    "    \"\"\"Base class for refactoring agents with retry logic\"\"\"\n",
    "    \n",
    "    def __init__(self, name: str, system_prompt: str):\n",
    "        self.name = name\n",
    "        self.system_prompt = system_prompt\n",
    "        self.client = client\n",
    "        self.model_name = MODEL_NAME\n",
    "        self.retry_config = retry_config\n",
    "    \n",
    "    def call(self, prompt: str, context: Dict = None) -> str:\n",
    "        \"\"\"Call agent with prompt and context, with automatic retry on HTTP errors\"\"\"\n",
    "        full_prompt = f\"{self.system_prompt}\\n\\n{prompt}\"\n",
    "        \n",
    "        if context:\n",
    "            import json\n",
    "            full_prompt += f\"\\n\\nContext:\\n{json.dumps(context, indent=2)}\"\n",
    "        \n",
    "        # Retry logic with exponential backoff\n",
    "        max_attempts = self.retry_config.attempts\n",
    "        initial_delay = self.retry_config.initial_delay\n",
    "        exp_base = self.retry_config.exp_base\n",
    "        retry_codes = self.retry_config.http_status_codes\n",
    "        \n",
    "        for attempt in range(max_attempts):\n",
    "            try:\n",
    "                response = self.client.models.generate_content(\n",
    "                    model=self.model_name,\n",
    "                    contents=full_prompt\n",
    "                )\n",
    "                return response.text\n",
    "            except Exception as e:\n",
    "                error_str = str(e)\n",
    "                \n",
    "                # Check if this is a retryable HTTP error\n",
    "                is_retryable = any(str(code) in error_str for code in retry_codes)\n",
    "                is_last_attempt = (attempt == max_attempts - 1)\n",
    "                \n",
    "                if is_retryable and not is_last_attempt:\n",
    "                    # Calculate delay with exponential backoff\n",
    "                    delay = initial_delay * (exp_base ** attempt)\n",
    "                    print(f\"âš ï¸  {self.name}: HTTP error on attempt {attempt + 1}/{max_attempts}\")\n",
    "                    print(f\"   Retrying in {delay:.1f}s... (exponential backoff)\")\n",
    "                    time.sleep(delay)\n",
    "                else:\n",
    "                    # Non-retryable error or final attempt\n",
    "                    raise Exception(f\"{self.name} failed: {error_str}\")\n",
    "        \n",
    "        raise Exception(f\"{self.name}: Exhausted all {max_attempts} retry attempts\")\n",
    "\n",
    "print(\"âœ“ RefactoringAgent base class defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "299470b0",
   "metadata": {},
   "source": [
    "## Section 5: Phase 1 - Type Annotation System\n",
    "\n",
    "Implement the HITL type annotation workflow with automated testing and rollback."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d2e4b5f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Phase 1 Environment Ready\n",
      "  Working Directory: /Users/pierre/Library/CloudStorage/GoogleDrive-pierre@baume.org/My Drive/AI Agents Intensive/code\n",
      "  Analyzer Tool: analyze_solver_types.py\n",
      "  Target File: arc-dsl/solvers.py\n"
     ]
    }
   ],
   "source": [
    "# Setup environment for Phase 1\n",
    "import subprocess\n",
    "import re\n",
    "import ast\n",
    "from pathlib import Path\n",
    "import os\n",
    "\n",
    "# Ensure we're in the correct directory\n",
    "os.chdir('/Users/pierre/Library/CloudStorage/GoogleDrive-pierre@baume.org/My Drive/AI Agents Intensive/code')\n",
    "\n",
    "print(\"âœ“ Phase 1 Environment Ready\")\n",
    "print(f\"  Working Directory: {os.getcwd()}\")\n",
    "print(f\"  Analyzer Tool: analyze_solver_types.py\")\n",
    "print(f\"  Target File: arc-dsl/solvers.py\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "556b98e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Phase 1 Tools Defined\n"
     ]
    }
   ],
   "source": [
    "def analyze_solver_types_tool(solver_name: str) -> dict:\n",
    "    \"\"\"\n",
    "    Analyze a solver function and return type annotations.\n",
    "    \n",
    "    Args:\n",
    "        solver_name: Name of solver function (e.g., 'solve_67a3c6ac')\n",
    "    \n",
    "    Returns:\n",
    "        dict with 'variables', 'annotated_code', 'success' keys\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Run the analyzer\n",
    "        result = subprocess.run(\n",
    "            ['python', 'analyze_solver_types.py', solver_name],\n",
    "            capture_output=True,\n",
    "            text=True,\n",
    "            timeout=10\n",
    "        )\n",
    "        \n",
    "        if result.returncode != 0:\n",
    "            return {\n",
    "                'success': False,\n",
    "                'error': result.stderr,\n",
    "                'solver': solver_name\n",
    "            }\n",
    "        \n",
    "        # Parse output to extract variable types and annotated code\n",
    "        output = result.stdout\n",
    "        lines = output.split('\\n')\n",
    "        \n",
    "        # Extract variables section\n",
    "        variables = {}\n",
    "        in_variables = False\n",
    "        for line in lines:\n",
    "            if line.startswith('Variables ('):\n",
    "                in_variables = True\n",
    "                continue\n",
    "            elif in_variables:\n",
    "                if line.startswith('Has Callables:') or line.strip() == '':\n",
    "                    break\n",
    "                # Variable lines are indented and have format \"  var: type\"\n",
    "                if ':' in line and line.startswith('  '):\n",
    "                    parts = line.strip().split(':', 1)\n",
    "                    if len(parts) == 2:\n",
    "                        var, vtype = parts\n",
    "                        variables[var.strip()] = vtype.strip()\n",
    "        \n",
    "        # Extract annotated code section\n",
    "        annotated_code = \"\"\n",
    "        in_code = False\n",
    "        skip_next_separator = False\n",
    "        \n",
    "        for i, line in enumerate(lines):\n",
    "            if 'Generated Annotated Code:' in line:\n",
    "                in_code = True\n",
    "                skip_next_separator = True\n",
    "                continue\n",
    "            elif in_code:\n",
    "                # Skip the separator line (====)\n",
    "                if skip_next_separator and line.startswith('===='):\n",
    "                    skip_next_separator = False\n",
    "                    continue\n",
    "                # Stop at next separator or HITL section\n",
    "                if line.startswith('====') or 'HITL Refactoring Script Info:' in line:\n",
    "                    break\n",
    "                # Collect code lines\n",
    "                annotated_code += line + '\\n'\n",
    "        \n",
    "        return {\n",
    "            'success': True,\n",
    "            'solver': solver_name,\n",
    "            'variables': variables,\n",
    "            'annotated_code': annotated_code.strip(),\n",
    "            'variable_count': len(variables)\n",
    "        }\n",
    "        \n",
    "    except subprocess.TimeoutExpired:\n",
    "        return {\n",
    "            'success': False,\n",
    "            'error': 'Analysis timed out',\n",
    "            'solver': solver_name\n",
    "        }\n",
    "    except Exception as e:\n",
    "        return {\n",
    "            'success': False,\n",
    "            'error': str(e),\n",
    "            'solver': solver_name\n",
    "        }\n",
    "\n",
    "\n",
    "def get_annotation_progress_tool() -> dict:\n",
    "    \"\"\"\n",
    "    Count how many solvers have been annotated with type hints.\n",
    "    \n",
    "    Returns:\n",
    "        dict with 'total', 'annotated', 'remaining', 'percent' keys\n",
    "    \"\"\"\n",
    "    try:\n",
    "        with open('arc-dsl/solvers.py', 'r') as f:\n",
    "            content = f.read()\n",
    "        \n",
    "        # Count all solver functions\n",
    "        total = len(re.findall(r'^def solve_\\w+\\(', content, re.MULTILINE))\n",
    "        \n",
    "        # Count annotated functions (have type hints)\n",
    "        annotated = len(re.findall(r'^def solve_\\w+\\(I: Grid\\) -> Grid:', content, re.MULTILINE))\n",
    "        \n",
    "        remaining = total - annotated\n",
    "        percent = round((annotated / total * 100), 1) if total > 0 else 0\n",
    "        \n",
    "        return {\n",
    "            'total': total,\n",
    "            'annotated': annotated,\n",
    "            'remaining': remaining,\n",
    "            'percent': percent\n",
    "        }\n",
    "    except Exception as e:\n",
    "        return {\n",
    "            'total': 0,\n",
    "            'annotated': 0,\n",
    "            'remaining': 0,\n",
    "            'percent': 0,\n",
    "            'error': str(e)\n",
    "        }\n",
    "\n",
    "\n",
    "def get_next_batch_tool(batch_size: int = 10, start_line: int = 1) -> dict:\n",
    "    \"\"\"\n",
    "    Get next batch of unannotated solver functions.\n",
    "    \n",
    "    Args:\n",
    "        batch_size: Number of solvers to return\n",
    "        start_line: Starting line number to search from\n",
    "    \n",
    "    Returns:\n",
    "        dict with 'batch' (list of solver names) and 'line_ranges' (dict of {name: (start, end)})\n",
    "    \"\"\"\n",
    "    try:\n",
    "        with open('arc-dsl/solvers.py', 'r') as f:\n",
    "            lines = f.readlines()\n",
    "        \n",
    "        batch = []\n",
    "        line_ranges = {}\n",
    "        \n",
    "        i = max(0, start_line - 1)  # Convert to 0-indexed\n",
    "        while i < len(lines) and len(batch) < batch_size:\n",
    "            line = lines[i]\n",
    "            \n",
    "            # Look for unannotated solver functions\n",
    "            match = re.match(r'^def (solve_\\w+)\\(I\\):', line)\n",
    "            if match:\n",
    "                solver_name = match.group(1)\n",
    "                start = i + 1  # Convert back to 1-indexed\n",
    "                \n",
    "                # Find end of function (next def or end of file)\n",
    "                end = start\n",
    "                for j in range(i + 1, len(lines)):\n",
    "                    if lines[j].startswith('def '):\n",
    "                        end = j  # Line before next function\n",
    "                        break\n",
    "                else:\n",
    "                    end = len(lines)\n",
    "                \n",
    "                batch.append(solver_name)\n",
    "                line_ranges[solver_name] = (start, end)\n",
    "            \n",
    "            i += 1\n",
    "        \n",
    "        return {\n",
    "            'batch': batch,\n",
    "            'line_ranges': line_ranges\n",
    "        }\n",
    "    except Exception as e:\n",
    "        return {\n",
    "            'batch': [],\n",
    "            'line_ranges': {},\n",
    "            'error': str(e)\n",
    "        }\n",
    "\n",
    "print(\"âœ“ Phase 1 Tools Defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "12c33045",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Type Annotation Agent Created\n",
      "  Agent name: Type Annotation Agent\n",
      "  Model: gemini-2.0-flash-exp\n"
     ]
    }
   ],
   "source": [
    "# Create Type Annotation Agent\n",
    "type_annotation_agent = RefactoringAgent(\n",
    "    name=\"Type Annotation Agent\",\n",
    "    system_prompt=\"\"\"You are a specialized Python type annotation expert for the ARC-DSL project.\n",
    "\n",
    "Your task: Add type hints to solver functions using the strict type system:\n",
    "- Grid (2D color grid)\n",
    "- Piece (region of grid)\n",
    "- Object (connected component)\n",
    "- Objects (collection of Objects)\n",
    "- Indices (set of cell positions)\n",
    "- Integer (color or count)\n",
    "- IntegerSet (set of integers)\n",
    "\n",
    "CRITICAL RULES:\n",
    "1. ONLY add type annotations - DO NOT rewrite logic\n",
    "2. Keep every line of code exactly as-is\n",
    "3. Function signature: def solve_X(I: Grid) -> Grid:\n",
    "4. Variable annotations: var: Type = expression\n",
    "5. Use the EXACT variable names from the original code\n",
    "6. Preserve all whitespace and formatting\n",
    "7. Use inferred types from analyze_solver_types.py output\n",
    "8. Return complete annotated function\n",
    "9. No explanations - only code\n",
    "\n",
    "Example transformation:\n",
    "BEFORE:\n",
    "def solve_67a3c6ac(I):\n",
    "    O = vmirror(I)\n",
    "    return O\n",
    "\n",
    "AFTER:\n",
    "def solve_67a3c6ac(I: Grid) -> Grid:\n",
    "    O: Grid = vmirror(I)\n",
    "    return O\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "print(\"âœ“ Type Annotation Agent Created\")\n",
    "print(f\"  Agent name: {type_annotation_agent.name}\")\n",
    "print(f\"  Model: {type_annotation_agent.model_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84f69210",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Phase 1 Workflow Function Ready\n"
     ]
    }
   ],
   "source": [
    "import ast\n",
    "import time\n",
    "from IPython.display import clear_output\n",
    "\n",
    "def run_type_annotation_test(batch_size: int = 10, start_line: int = 1, end_line: int = None):\n",
    "    \"\"\"\n",
    "    Interactive workflow for type annotation with human approval.\n",
    "    Uses clear_output to maintain a clean interface and avoid duplication artifacts.\n",
    "    \n",
    "    Args:\n",
    "        batch_size: Number of solvers to process\n",
    "        start_line: Starting line in solvers.py\n",
    "        end_line: Ending line (None = search to end of file)\n",
    "    \"\"\"\n",
    "    # Use the pre-created type_annotation_agent (defined in previous cell)\n",
    "    # Note: Each call to agent.call() is stateless - no conversation history contamination\n",
    "    \n",
    "    # Get initial progress\n",
    "    progress = get_annotation_progress_tool()\n",
    "    \n",
    "    # Get batch of unannotated solvers\n",
    "    batch_info = get_next_batch_tool(batch_size, start_line)\n",
    "    batch = batch_info['batch']\n",
    "    line_ranges = batch_info['line_ranges']\n",
    "    \n",
    "    # Filter by end_line if specified\n",
    "    if end_line is not None:\n",
    "        batch = [s for s in batch if line_ranges[s][0] <= end_line]\n",
    "    \n",
    "    print(f\"\\nðŸŽ¯ Processing batch of {len(batch)} solvers (lines {start_line}-{end_line or 'EOF'})\")\n",
    "    print(f\"   Solvers: {', '.join(batch[:5])}{'...' if len(batch) > 5 else ''}\\n\")\n",
    "    \n",
    "    results = {\n",
    "        'processed': 0,\n",
    "        'approved': 0,\n",
    "        'refined': 0,\n",
    "        'skipped': 0,\n",
    "        'failed': 0,\n",
    "        'details': []\n",
    "    }\n",
    "    \n",
    "    for i, solver_name in enumerate(batch):\n",
    "        clear_output(wait=True)\n",
    "        print(f\"\\nðŸ“Š Batch Progress: {results['processed']}/{len(batch)} processed\")\n",
    "        print(f\"   Approved: {results['approved']} | Refined: {results['refined']} | Skipped: {results['skipped']} | Failed: {results['failed']}\")\n",
    "        \n",
    "        print(f\"\\n{'='*70}\")\n",
    "        print(f\"[{i}/{len(batch)}] Processing: {solver_name}\")\n",
    "        print(f\"{'='*70}\")\n",
    "        \n",
    "        # Analyze the solver\n",
    "        analysis = analyze_solver_types_tool(solver_name)\n",
    "        \n",
    "        if not analysis['success']:\n",
    "            print(f\"âŒ Analysis failed: {analysis.get('error', 'Unknown error')}\")\n",
    "            results['failed'] += 1\n",
    "            results['details'].append({\n",
    "                'solver': solver_name,\n",
    "                'status': 'failed',\n",
    "                'error': analysis.get('error')\n",
    "            })\n",
    "            time.sleep(2) # Pause to let user see error\n",
    "            continue\n",
    "        \n",
    "        # Check if analysis returned valid results\n",
    "        if not analysis.get('annotated_code') or analysis.get('annotated_code', '').strip() == '':\n",
    "            print(f\"âš ï¸  Analysis returned empty annotated code - skipping\")\n",
    "            results['failed'] += 1\n",
    "            results['details'].append({\n",
    "                'solver': solver_name,\n",
    "                'status': 'failed',\n",
    "                'error': 'Empty annotated code from analyzer'\n",
    "            })\n",
    "            time.sleep(2)\n",
    "            continue\n",
    "        \n",
    "        # Display analysis results\n",
    "        print(f\"\\nðŸ“‹ Analysis Results:\")\n",
    "        print(f\"   Variables: {analysis['variable_count']}\")\n",
    "        for var, vtype in list(analysis['variables'].items())[:10]:\n",
    "            print(f\"      {var}: {vtype}\")\n",
    "        if len(analysis['variables']) > 10:\n",
    "            print(f\"      ... and {len(analysis['variables']) - 10} more\")\n",
    "        \n",
    "        # Show proposed annotated code\n",
    "        print(f\"\\nðŸ“ Proposed Annotated Code:\")\n",
    "        print(\"â”€\" * 70)\n",
    "        print(analysis['annotated_code'][:500])\n",
    "        if len(analysis['annotated_code']) > 500:\n",
    "            print(\"...\")\n",
    "        print(\"â”€\" * 70)\n",
    "        \n",
    "        # Human-in-the-loop decision\n",
    "        max_refinement_attempts = 3\n",
    "        refinement_count = 0\n",
    "        \n",
    "        while refinement_count < max_refinement_attempts:\n",
    "            decision = input(f\"\\nâ“ Decision [a=approve, r=refine, s=skip, q=abort]: \").strip().lower()\n",
    "            \n",
    "            if decision == 'q':\n",
    "                print(\"â›” Workflow aborted by user\")\n",
    "                return results\n",
    "            \n",
    "            elif decision == 's':\n",
    "                print(f\"â­ï¸  Skipping {solver_name}\")\n",
    "                results['skipped'] += 1\n",
    "                results['details'].append({\n",
    "                    'solver': solver_name,\n",
    "                    'status': 'skipped'\n",
    "                })\n",
    "                time.sleep(1) # Add delay to see skip message\n",
    "                break\n",
    "            \n",
    "            elif decision == 'r':\n",
    "                refinement_count += 1\n",
    "                print(f\"\\nðŸ”„ Refinement attempt {refinement_count}/{max_refinement_attempts}\")\n",
    "                \n",
    "                feedback = input(\"   Provide refinement instructions: \").strip()\n",
    "                \n",
    "                # Use the agent to refine based on feedback\n",
    "                refinement_prompt = f\"\"\"\n",
    "Based on this feedback: \"{feedback}\"\n",
    "\n",
    "Re-annotate this function:\n",
    "{analysis['annotated_code']}\n",
    "\n",
    "Apply the requested refinements while following all type annotation rules.\n",
    "\"\"\"\n",
    "                \n",
    "                # Call the pre-created agent (each call is stateless)\n",
    "                response = type_annotation_agent.call(refinement_prompt)\n",
    "                \n",
    "                print(f\"\\nðŸ“ Refined Code:\")\n",
    "                print(\"â”€\" * 70)\n",
    "                print(response[:500])\n",
    "                if len(response) > 500:\n",
    "                    print(\"...\")\n",
    "                print(\"â”€\" * 70)\n",
    "                \n",
    "                # Update the proposal\n",
    "                analysis['annotated_code'] = response\n",
    "                \n",
    "                continue  # Ask for decision again\n",
    "            \n",
    "            elif decision == 'a':\n",
    "                print(f\"\\nâœ… Approved: {solver_name}\")\n",
    "                \n",
    "                # Double-check we have valid code before applying\n",
    "                if not analysis['annotated_code'] or analysis['annotated_code'].strip() == '':\n",
    "                    print(f\"   âŒ Cannot apply: annotated code is empty!\")\n",
    "                    results['failed'] += 1\n",
    "                    results['details'].append({\n",
    "                        'solver': solver_name,\n",
    "                        'status': 'failed',\n",
    "                        'error': 'Empty annotated code'\n",
    "                    })\n",
    "                    break\n",
    "                \n",
    "                # Apply the annotation using AST-based replacement\n",
    "                try:\n",
    "                    with open('arc-dsl/solvers.py', 'r') as f:\n",
    "                        file_content = f.read()\n",
    "                    \n",
    "                    # Use AST to find exact function boundaries\n",
    "                    tree = ast.parse(file_content)\n",
    "                    target_func = None\n",
    "                    \n",
    "                    for node in ast.walk(tree):\n",
    "                        if isinstance(node, ast.FunctionDef) and node.name == solver_name:\n",
    "                            target_func = node\n",
    "                            break\n",
    "                    \n",
    "                    if target_func is None:\n",
    "                        print(f\"   âš ï¸  Function {solver_name} not found in AST\")\n",
    "                        results['failed'] += 1\n",
    "                        results['details'].append({\n",
    "                            'solver': solver_name,\n",
    "                            'status': 'failed',\n",
    "                            'error': 'Function not found in AST'\n",
    "                        })\n",
    "                        break\n",
    "                    \n",
    "                    # Get exact line boundaries (1-indexed)\n",
    "                    start_line_idx = target_func.lineno - 1  # Convert to 0-indexed\n",
    "                    end_line_idx = target_func.end_lineno  # This is already the correct end\n",
    "                    \n",
    "                    # Split content into lines\n",
    "                    lines = file_content.split('\\n')\n",
    "                    \n",
    "                    # Replace the function (no extra empty line)\n",
    "                    new_lines = (\n",
    "                        lines[:start_line_idx] +\n",
    "                        [analysis['annotated_code']] +\n",
    "                        lines[end_line_idx:]\n",
    "                    )\n",
    "                    \n",
    "                    new_content = '\\n'.join(new_lines)\n",
    "                    \n",
    "                    # Write back\n",
    "                    with open('arc-dsl/solvers.py', 'w') as f:\n",
    "                        f.write(new_content)\n",
    "                    \n",
    "                    print(f\"   âœ“ Type annotations applied successfully\", flush=True)\n",
    "                    \n",
    "                    # Run regression tests\n",
    "                    print(f\"\\nðŸ§ª Running regression tests...\", flush=True)\n",
    "                    \n",
    "                    # Test 1: Run tests.py\n",
    "                    test_result = subprocess.run(\n",
    "                        ['python', 'tests.py'],\n",
    "                        cwd='arc-dsl',\n",
    "                        capture_output=True,\n",
    "                        text=True,\n",
    "                        timeout=30\n",
    "                    )\n",
    "                    \n",
    "                    tests_passed = test_result.returncode == 0\n",
    "                    \n",
    "                    if tests_passed:\n",
    "                        print(f\"   âœ… tests.py passed\")\n",
    "                    else:\n",
    "                        print(f\"   âŒ tests.py failed\")\n",
    "                        if test_result.stderr:\n",
    "                            print(f\"      Error: {test_result.stderr[:200]}\")\n",
    "                    \n",
    "                    # Test 2: Run main.py and check solver count\n",
    "                    main_result = subprocess.run(\n",
    "                        ['python', 'main.py'],\n",
    "                        cwd='arc-dsl',\n",
    "                        capture_output=True,\n",
    "                        text=True,\n",
    "                        timeout=60\n",
    "                    )\n",
    "                    \n",
    "                    solved_count = None\n",
    "                    if main_result.returncode == 0 and main_result.stdout:\n",
    "                        # Parse \"Tasks solved correctly: 390 out of 1000\"\n",
    "                        import re\n",
    "                        match = re.search(r'(\\d+)\\s+out of\\s+(\\d+)', main_result.stdout)\n",
    "                        if match:\n",
    "                            solved_count = int(match.group(1))\n",
    "                            expected_min = 390  # Baseline from initial run\n",
    "                            \n",
    "                            if solved_count >= expected_min:\n",
    "                                print(f\"   âœ… main.py passed ({solved_count}/1000 tasks solved)\")\n",
    "                            else:\n",
    "                                print(f\"   âŒ main.py regression ({solved_count}/1000, expected â‰¥{expected_min})\")\n",
    "                                tests_passed = False\n",
    "                    else:\n",
    "                        print(f\"   âš ï¸  main.py couldn't be verified\")\n",
    "                        # Don't fail on main.py verification issues\n",
    "                    \n",
    "                    # Decision based on test results\n",
    "                    if tests_passed:\n",
    "                        print(f\"\\n   âœ… All regression checks passed - changes kept\")\n",
    "                        \n",
    "                        results['approved'] += 1\n",
    "                        if refinement_count > 0:\n",
    "                            results['refined'] += 1\n",
    "                        \n",
    "                        results['details'].append({\n",
    "                            'solver': solver_name,\n",
    "                            'status': 'approved',\n",
    "                            'variables': analysis['variable_count'],\n",
    "                            'refinements': refinement_count,\n",
    "                            'tests': 'passed',\n",
    "                            'solved_count': solved_count\n",
    "                        })\n",
    "                        time.sleep(1) # Pause to let user see success\n",
    "                        break  # Exit decision loop after successful approval\n",
    "                    else:\n",
    "                        print(f\"\\n   âŒ REGRESSION DETECTED - rolling back changes\")\n",
    "                        \n",
    "                        # Restore original content\n",
    "                        with open('arc-dsl/solvers.py', 'w') as f:\n",
    "                            f.write(file_content)\n",
    "                        print(f\"   â†©ï¸  Original code restored\")\n",
    "                        \n",
    "                        results['failed'] += 1\n",
    "                        results['details'].append({\n",
    "                            'solver': solver_name,\n",
    "                            'status': 'failed',\n",
    "                            'error': 'Regression: tests failed',\n",
    "                            'test_output': test_result.stderr[:500] if test_result.stderr else test_result.stdout[:500],\n",
    "                            'solved_count': solved_count\n",
    "                        })\n",
    "                        time.sleep(3) # Pause to let user see failure\n",
    "                        break  # Exit decision loop after failed test\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    print(f\"   âŒ Failed to apply annotations: {e}\")\n",
    "                    results['failed'] += 1\n",
    "                    results['details'].append({\n",
    "                        'solver': solver_name,\n",
    "                        'status': 'failed',\n",
    "                        'error': str(e)\n",
    "                    })\n",
    "                    time.sleep(2)\n",
    "                \n",
    "                break\n",
    "            \n",
    "            else:\n",
    "                print(\"âŒ Invalid choice. Please enter 'a', 'r', 's', or 'q'\")\n",
    "        \n",
    "        results['processed'] += 1\n",
    "    \n",
    "    print(f\"\\n\\n{'='*70}\")\n",
    "    print(\"ðŸ“Š WORKFLOW SUMMARY\")\n",
    "    print(f\"{'='*70}\")\n",
    "    print(f\"Processed:  {results['processed']}/{len(batch)} solvers\")\n",
    "    print(f\"âœ… Approved: {results['approved']}\")\n",
    "    print(f\"ðŸ”„ Refined:  {results['refined']}\")\n",
    "    print(f\"â­ï¸  Skipped:  {results['skipped']}\")\n",
    "    print(f\"âŒ Failed:   {results['failed']}\")\n",
    "    print(f\"\\nOverall Progress:\")\n",
    "    print(f\"   Annotated: {final_progress['annotated']}/{final_progress['total']} ({final_progress['percent']}%)\")\n",
    "    print(f\"   Remaining: {final_progress['remaining']}\")\n",
    "    print(f\"{'='*70}\\n\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "print(\"âœ“ Phase 1 Workflow Function Ready\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1cb03245",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ“Š Batch Progress: 1/10 processed\n",
      "   Approved: 1 | Refined: 0 | Skipped: 0 | Failed: 0\n",
      "\n",
      "======================================================================\n",
      "[1/10] Processing: solve_25ff71a9\n",
      "======================================================================\n",
      "\n",
      "ðŸ“‹ Analysis Results:\n",
      "   Variables: 4\n",
      "      I: Grid\n",
      "      x1: Objects\n",
      "      x2: Any\n",
      "      O: Grid\n",
      "\n",
      "ðŸ“ Proposed Annotated Code:\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "def solve_25ff71a9(I: Grid) -> Grid:\n",
      "    x1: Objects = objects(I, T, T, T)\n",
      "    x2: Any = first(x1)\n",
      "    O: Grid = move(I, x2, DOWN)\n",
      "    return O\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "â›” Workflow aborted by user\n",
      "â›” Workflow aborted by user\n"
     ]
    }
   ],
   "source": [
    "# Run Phase 1 Type Annotation Test\n",
    "# Process small batch for testing (lines 5-73 contain ~10 solvers)\n",
    "\n",
    "results = run_type_annotation_test(\n",
    "    batch_size=10,\n",
    "    start_line=5,\n",
    "    end_line=300\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
